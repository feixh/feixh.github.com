<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<div id="TOC">
<ul>
<li><a href="#deep-learning-meets-geometry">Deep Learning Meets Geometry</a><ul>
<li><a href="#raynet-learning-volumetric-3d-reconstruction-with-ray-potentials">RayNet: Learning Volumetric 3d Reconstruction with Ray Potentials</a></li>
<li><a href="#learning-a-multi-view-stereo-machine">Learning a Multi-View Stereo Machine</a></li>
<li><a href="#factoring-shape-pose-and-layout-from-the-2d-image-of-a-3d-scene">Factoring Shape, Pose, and Layout from the 2D Image of a 3D Scene</a></li>
<li><a href="#learning-category-specific-mesh-reconstruction-from-image-collections">Learning Category-Specific Mesh Reconstruction from Image Collections</a></li>
<li><a href="#multi-view-consistency-as-supervisory-signal-for-learning-shape-and-pose-prediction">Multi-view Consistency as Supervisory Signal for Learning Shape and Pose Prediction</a></li>
<li><a href="#neural-3d-mesh-renderer">Neural 3D Mesh Renderer</a></li>
<li><a href="#multi-view-supervision-for-single-view-reconstruction-via-differentiable-ray-consistency">Multi-view supervision for single-view reconstruction via differentiable ray consistency</a></li>
<li><a href="#unsupervised-learning-of-depth-and-ego-motion-from-video">Unsupervised Learning of Depth and Ego-Motion from Video</a></li>
<li><a href="#perspective-transformer-nets-learning-single-view-3d-object-reconstruction-without-3d-supervision">Perspective Transformer Nets: Learning Single-View 3D Object Reconstruction without 3D Supervision</a></li>
<li><a href="#unsupervised-learning-of-3d-structure-from-images">Unsupervised Learning of 3D Structure from Images</a></li>
<li><a href="#octree-generation-networks-efficient-convolutional-architectures-for-high-resolution-3d-outputs">Octree generation networks: Efficient convolutional architectures for high-resolution 3d outputs</a></li>
<li><a href="#learning-a-probabilistic-latent-space-of-object-shapes-via-3d-generative-adversarial-modeling">Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling</a></li>
<li><a href="#marrnet-3d-shape-reconstruction-via-2.5d-sketches">MarrNet: 3D Shape Reconstruction via 2.5D Sketches</a></li>
<li><a href="#pix3d-dataset-and-methods-for-single-image-3d-shape-modeling">Pix3D: Dataset and Methods for Single-Image 3D Shape Modeling</a></li>
<li><a href="#pixels-voxels-and-views-a-study-of-shape-representations-for-single-view-3d-object-shape-prediction">Pixels, voxels, and views: A study of shape representations for single view 3D object shape prediction</a></li>
<li><a href="#im2struct-recovering-3d-shape-structure-from-a-single-rgb-image">Im2Struct: Recovering 3D Shape Structure from a Single RGB Image</a></li>
<li><a href="#attentional-shapecontextnet-for-point-cloud-recognition">Attentional ShapeContextNet for Point Cloud Recognition</a></li>
<li><a href="#a-variational-u-net-for-conditional-appearance-and-shape-generation">A Variational U-Net for Conditional Appearance and Shape Generation</a></li>
<li><a href="#pointgrid-a-deep-network-for-3d-shape-understanding">** PointGrid: A Deep Network for 3D Shape Understanding</a></li>
<li><a href="#learning-to-estimate-3d-human-pose-and-shape-from-a-single-color-image">Learning to Estimate 3D Human Pose and Shape from a Single Color Image</a></li>
<li><a href="#d-object-detection-with-latent-support-surfaces">* 3D Object Detection with Latent Support Surfaces</a></li>
<li><a href="#reconstructing-thin-structures-of-manifold-surfaces-by-integrating-spatial-curves">Reconstructing Thin Structures of Manifold Surfaces by Integrating Spatial Curves</a></li>
<li><a href="#learning-less-is-more---6d-camera-localization-via-3d-surface-regression">Learning Less is More - 6D Camera Localization via 3D Surface Regression</a></li>
<li><a href="#d-registration-of-curves-and-surfaces-using-local-differential-information">3D Registration of Curves and Surfaces using Local Differential Information</a></li>
<li><a href="#geonet-geometric-neural-network-for-joint-depth-and-surface-normal-estimation">GeoNet: Geometric Neural Network for Joint Depth and Surface Normal Estimation</a></li>
<li><a href="#codeslam-----learning-a-compact-optimisable-representation-for-dense-visual-slam">CodeSLAM --- Learning a Compact, Optimisable Representation for Dense Visual SLAM</a></li>
<li><a href="#deep-marching-cubes-learning-explicit-surface-representations">** Deep Marching Cubes: Learning Explicit Surface Representations</a></li>
<li><a href="#end-to-end-recovery-of-human-shape-and-pose">** End-to-end recovery of Human Shape and Pose</a></li>
<li><a href="#planenet-piece-wise-planar-reconstruction-froma-single-rgb-image">**PlaneNet: Piece-wise Planar Reconstruction froma Single RGB Image</a></li>
<li><a href="#image-collection-pop-up-3d-reconstruction-and-clustering-of-rigid-and-non-rigid-categories">**Image Collection Pop-up: 3D Reconstruction and Clustering of Rigid and Non-Rigid Categories</a></li>
<li><a href="#extreme-3d-face-reconstruction-looking-past-occlusions">**Extreme 3D Face Reconstruction: Looking Past Occlusions</a></li>
<li><a href="#section"></a></li>
</ul></li>
<li><a href="#mapping-and-planning">Mapping and Planning</a><ul>
<li><a href="#unifying-map-and-landmark-based-representations-for-visual-navigation">Unifying Map and Landmark Based Representations for Visual Navigation</a></li>
<li><a href="#cognitive-mapping-and-planning-for-visual-navigation">Cognitive Mapping and Planning for Visual Navigation</a></li>
</ul></li>
<li><a href="#slam-meets-deep-learning">SLAM Meets Deep Learning</a><ul>
<li><a href="#unsupervised-learning-of-depth-and-ego-motion-from-monocular-video-using-3d-geometric-constraints">Unsupervised Learning of Depth and Ego-Motion from Monocular Video Using 3D Geometric Constraints</a></li>
<li><a href="#v2v-posenet-voxel-to-voxel-prediction-network-for-accurate-3d-hand-and-human-pose-estimation-from-a-single-depth-map">V2V-PoseNet: Voxel-to-Voxel Prediction Network for Accurate 3D Hand and Human Pose Estimation from a Single Depth Map</a></li>
<li><a href="#real-time-monocular-depth-estimation-using-synthetic-data-with-domain-adaptation-via-image-style-transfer">Real-Time Monocular Depth Estimation using Synthetic Data with Domain Adaptation via Image Style Transfer</a></li>
<li><a href="#megadepth-learning-single-view-depth-prediction-from-internet-photos">MegaDepth: Learning Single-View Depth Prediction from Internet Photos</a></li>
<li><a href="#learning-depth-from-monocular-videos-using-direct-methods">Learning Depth from Monocular Videos using Direct Methods</a></li>
<li><a href="#inverse-compositional-spatial-transformer-networks">Inverse Compositional Spatial Transformer Networks</a></li>
<li><a href="#clkn-cascaded-lucas-kanade-networks-for-image-alignment.-cvpr-17.">CLKN: Cascaded Lucas-Kanade networks for image alignment. CVPR, 17.</a></li>
<li><a href="#geonet-unsupervised-learning-of-dense-depth-optical-flow-and-camera-pose">GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose</a></li>
<li><a href="#unsupervised-learning-of-monocular-depth-estimation-and-visual-odometry-with-deep-feature-reconstruction">Unsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction</a></li>
<li><a href="#unsupervised-monocular-depth-estimation-with-left-right-consistency">Unsupervised Monocular Depth Estimation with Left-Right Consistency</a></li>
<li><a href="#d-rcnn-instance-level-3d-object-reconstruction-via-render-and-compare">** 3D-RCNN: Instance-level 3D Object Reconstruction via Render-and-Compare</a></li>
<li><a href="#feature-space-optimization-for-semantic-video-segmentation">Feature Space Optimization for Semantic Video Segmentation</a></li>
</ul></li>
<li><a href="#the-basics">The Basics</a><ul>
<li><a href="#spatial-transformer-networks">Spatial Transformer Networks</a></li>
<li><a href="#generative-adversarial-nets">Generative Adversarial Nets</a></li>
<li><a href="#auto-encoding-variational-bayes">Auto-Encoding Variational Bayes</a></li>
<li><a href="#stochastic-backpropagation-and-approximate-inference-in-deep-generative-models">Stochastic Backpropagation and Approximate Inference in Deep Generative Models</a></li>
<li><a href="#an-introduction-to-variational-methods-for-graphical-models">An Introduction to Variational Methods for Graphical Models</a></li>
<li><a href="#uncertainty-in-deep-learning">Uncertainty in Deep Learning</a></li>
</ul></li>
</ul>
</div>
<!--
# Paper Notes

Xiaohan Fei

Summary of topics:

- Geometry aided deep learning for reconstruction, either in the form of a global voxel grid or view-specific depth/disparity maps.
- Deep learning aided SLAM/VO methods. Replace part of or the whole pipeline of traditional SLAM/VO methods with deep learning approaches. Such as ego motion estimation, depth estimation, feature matching, etc.
- Mapping and planning: with good map representation thanks to deep learning, can we plan better?
-->
<h2 id="deep-learning-meets-geometry">Deep Learning Meets Geometry</h2>
<p>The idea behind this line of work is: We should take multi-view geometry into consideration, which is well studied but not fully explored in deep learning based frameworks.</p>
<h3 id="raynet-learning-volumetric-3d-reconstruction-with-ray-potentials">RayNet: Learning Volumetric 3d Reconstruction with Ray Potentials</h3>
<p>CVPR 2018</p>
<p><a href="http://www.cvlibs.net/publications/Paschalidou2018CVPR.pdf" class="uri">http://www.cvlibs.net/publications/Paschalidou2018CVPR.pdf</a></p>
<ul>
<li>Problem: Depth estimation given multiple views and relative motion</li>
<li>CNNs can handle large appearance variation but do not model the physics of image formation which is modelled by MRF with ray potentials</li>
<li>RayNet: integrate a CNN (viewpoint invariant feature representations) with an MRF (explicityly model perspective projection and occlusion)</li>
<li>Difficulties: Naive backpropagation through the unrolled MRF is intractable due to the large amount of messages needed to be stored during training - solution: Stochastic ray sampling approach allows efficient BP - MRF: act as regularizer and improve the output of both CNN and joint model <img src="2018-03-30-16-55-38.png" alt="illustration" /></li>
<li>MRF with ray potentials is unrolled as 3 layers since observation shows MRF usually converges after 3 iterations, notation and formulation are the following, for more details see Ulusoy's 3DV 15 paper.</li>
</ul>
<p><strong>Ray Potential MRF Formulation</strong>:</p>
<p>Each voxel <span class="math inline">\(i \in \Omega\)</span> is assigned a binary occupancy variable <span class="math inline">\(o_i\)</span>. Ordered set <span class="math inline">\(\mathbf{o}_r\doteq(o_1^r, o_2^r,\cdots,o_{N_r}^r)\)</span> denotes voxels interecting a single ray <span class="math inline">\(r \in \mathcal{R}\)</span>. Joint distribution over all the occupancy variables <span class="math inline">\(\mathbf{o}=\{o_i| i \in \Omega\}\)</span> is:</p>
<p><span class="math display">\[
p(\mathbf{o})=\frac{1}{Z}\prod_{i \in \Omega} \phi_i(o_i) \prod_{r \in \mathcal R} \psi_r(\mathbf{o}_r)
\]</span></p>
<p>where the first term is unary potential encoding prior <span class="math inline">\(\phi_i(o_i)=\gamma^{o_i}(1-\gamma)^{1-o_i}\)</span> and the second term is ray potential <span class="math inline">\(\psi_r(\mathbf o_r)\prod_{j \lt i} (1-o_j^r)s_i^r\)</span> where <span class="math inline">\(s_i^r\)</span> is the probability of the visible surface along ray <span class="math inline">\(r\)</span> beging located at voxel <span class="math inline">\(i\)</span>. <span class="math inline">\(s_i^r\)</span> is noisy prediction from CNN. Loss w.r.t. ground truth depth requires depth estimation extracted from occupancy states:</p>
<p><span class="math display">\[
d_r = \sum_{i=1}^{N_r} o_i^r \prod_{j \lt i} (1-o_i^r) d_i^r
\]</span></p>
<p>is the depth over the set of depth values <span class="math inline">\(\mathbf{d}_r=\{d_1^r, \cdots, d_{N_r}^r\}\)</span> corresponding to set occupancy grid set <span class="math inline">\(\mathbf{o}_r\)</span> associated with ray <span class="math inline">\(r\)</span>.</p>
<p>Loss: L1 loss relative to the GT depth.</p>
<ul>
<li><p>As an inference tool, can we use convex relaxation which probably doesn't require this unroll thing.</p></li>
<li><p>From the experiments, it seems this approach is not significantly better than plance sweep with ZNCC/SAD (Christopher Hane real-time direct dense matching on fisheye images using plane-sweeping stereo, 3DV 14)?</p></li>
</ul>
<!-- 
- Related work:
    1. SurfaceNet: An End-to-end 3D Neural Network for Multiview Stereopsis. ICCV 17.
    2. Learned multi-patch similarity. ICCV 17.
    3. Towards probabilistic volumetric reconstruction using ray potentials. 3DV 15.
        - An old one, but 1st author A. O. Ulusoy has many good works on ray potential MRF.
    4. Learning a multi-view stereo machine. NIPS 17.
    Previous depth estimation network either 
        - doesn't consider the image formation process at all OR
        - makes use of projective geometry but **doesn't explicitly exploit occlusion relationships across viewpoints**
            - Multiple views are fed into a generic 3D CNN and the network directly maps multiple views to 3D reconstruction
        - all these methods require 3D supervision, to overcome various supervision signals are exploited
            - Tulsiani: Differentialable view consistency loss 
            - Rezende: Neural projection layer and black box renderer
            - Yan and Gwak: 2D silhouettes as supervision
        - RayNet imposes the physical constraints via MRF with ray potentials so that the network does not need to learn these constrains from data

-->
<hr />
<h3 id="learning-a-multi-view-stereo-machine">Learning a Multi-View Stereo Machine</h3>
<p>NIPS 2017</p>
<p><a href="https://arxiv.org/abs/1708.05375" class="uri">https://arxiv.org/abs/1708.05375</a></p>
<ul>
<li>Difference from other learning based reconstruction framework: - This one also exploits geometric cues of the underlying problem via projection/unprojection of features along viewing rays - Others solely rely on semantic cues such as 3D-R2N2 - This method also uses geometric constraints during inference while others only use such cues in training -- such left-right consistency</li>
<li>Showcase the system for object reconstruction -- objects are perfectly segmented and placed against white background - Compare to plane sweeping, visual hulls and other learnt stereo method - This method is able to reconstruct objects with fewer images than classical ones - And is able to better use camera pose (naive use of camera pose does not lead to expected gains) -- adding more views significantly improves performance - Generalize well to unseen object categories -- a benefit brought by geometric cues which is not fully explored in other learnt method</li>
</ul>
<h4 id="method">Method</h4>
<ul>
<li><p>Overall pipeline: 1. Produce dense feature maps <span class="math inline">\(\{F_i\}_{i=1}^n\)</span> for input images <span class="math inline">\(\{I_i\}_{i=1}^n\)</span> 2. Unproject the features into 3D feature grids <span class="math inline">\(\{G_i\}_{i=1}^n\)</span> by rastering the viewing rays with the known camera poses <span class="math inline">\(\{P_i\}_{i=1}^n\)</span> 3. Match the unprojected features using a recurrent neural network which processes the unprojected grids sequentially to produce a grid of local matching costs <span class="math inline">\(G^p\)</span> 4. Smooth the noisy cost volume <span class="math inline">\(G^p\)</span> by a 3D encoder-decoder network and generate a smoothed cost volume <span class="math inline">\(G^o\)</span>. The encoder-decoder mimics an optimization framework with data and smoothness terms. 5. The final output can be either a volumetric occupancy grid (Voxel LSM) or a grid of features whic is projected back into 2D feature maps <span class="math inline">\(\{O_i\}_{i=1}^n\)</span>, from which per view depth/disparity map can be constructed (Depth LSM). <img src="2018-04-05-12-56-14.png" /></p></li>
<li>Key components: differentiable projection/unprojection operators</li>
<li>2D Image Encoder: encoder-decoder network with skip connections (UNet) outputs feature maps given input images.</li>
<li>Differentiable Unprojection: - Given the k-th voxel, first project its center in world frame <span class="math inline">\(X_w^k\)</span> onto image plane via the camera intrinsics <span class="math inline">\(K\)</span> and extrinsics <span class="math inline">\([R|t]\in\mathrm{SE}(3)\)</span>, i.e., <span class="math inline">\(p_k&#39;=K[R|t]X_w^k\)</span>. - Use differentiable bilinear sampling to sample from the discrete feature grid to obtain the feature at <span class="math inline">\(X_w^k\)</span>. - Append depth value and ray direction at each sampling point so that surfaces can be infered even from a single image. (Impossible to match features if all one has is one image. By appending geometric features, one is essentially do single image depth estimation with both photometric and gometric features.)</li>
<li>Recurrent Grid Fusion: Sequentially fuse 3D feature grids <span class="math inline">\(\{G_i^f\}_{i=1}^n\)</span> into one <span class="math inline">\(G^p\)</span> by a GRU, which <em>mimics incremental matching in MVS where the hidden state of the GRU stores a running belief about the matching scores by matching features in the observations it has seen</em>. - Issue: as a sequential method the result depends on the order of input images. Randomly permute the input images and constrain the result to be the same to make network invariant to input order. - Other methods: Point-wise function such as max or average (see PointNet) is spatially too local. Concatenating all feature grids to feed a CNN scales linearly with number of inputs and only works for fixed number of inputs.</li>
<li>3D Grid Reasoning: Once the 3D feature grids are fused into one <span class="math inline">\(G^p\)</span>, a classical multi-view stereo appraoch would directly evaluate the photo-consistency at the grid locations by comparing the appearance of the individual views and extract the surface at voxels where the images agree. This step is modeled with a 3D UNet that transforms the fused grid <span class="math inline">\(G^p\)</span> to <span class="math inline">\(G^o\)</span>. - In the case of Voxel LSM where the final output is a 3D occupancy grid, the 3D UNet learns the mapping from feature grid to occupancy grid, which can be considered as <span class="math inline">\(G^o\)</span>. - In the case of per view depth/disparity estimation, the 3D feature grid can be used in the following differentiable projection modules such that view-based representation can be infered.</li>
<li><p>Differentiable Projection: Consider a certain view with parameters <span class="math inline">\(K\)</span> and <span class="math inline">\([R|t]\in \mathrm{SE}(3)\)</span>. For each viewing ray, uniformly sample its depth along the viewing direction (say we have n sampling point), stack the features along the channel dimension at the sampling point. <span class="math inline">\(1\times 1\)</span> convolution can be applied to the stacked features to reduce the dimension of features (projection) which are then used predict depth/disparity map.</p></li>
</ul>
<h4 id="experiments">Experiments</h4>
<ul>
<li>Voxel grid resolution: <span class="math inline">\(32 \times 32 \times 32\)</span></li>
<li>Input image size: <span class="math inline">\(224 \times 224\)</span></li>
<li>Batch size: 4</li>
<li>Each batch contains 4 views of an object</li>
</ul>
<h4 id="reference">Reference</h4>
<ul>
<li>[24, 4, 5] for GRU</li>
</ul>
<hr />
<h3 id="factoring-shape-pose-and-layout-from-the-2d-image-of-a-3d-scene">Factoring Shape, Pose, and Layout from the 2D Image of a 3D Scene</h3>
<p>CVPR 18</p>
<p><a href="https://arxiv.org/abs/1712.01812" class="uri">https://arxiv.org/abs/1712.01812</a></p>
<ul>
<li>Widely used representation for 3D scenes are not sufficient. Both of view-specific depth/normal maps or voxel grid do NOT distinguish objects -- the representation lacks semantics. - View-specific depth/normal maps: Invisible surfaces are not represented. - Voxel grid: The shape and pose of objects are mixed together in voxel representation. - Why a factored representation of shape and pose is important? With such a representation, we can manipulate objects in the scene while keeping other factors untouched. For instance, we can rotate the chair, move the chair, without deforming its shape.</li>
<li>Relation to other works: - RGB-D based shape completion: Geometry is freely available, and one only needs to infer the missing shapes. Much simpler. - CAD retrieval based methods: A dictionary of known shapes is available and one only needs to retrieve proper shapes and estimate its pose. - Other works which try to extend 2D detection to full 3D reconstruction: representation not as rich as this work. (mostly low-dimensional parametric shape)</li>
<li>Proposed representation is comprised of <em>layout</em> and <em>objects</em>: - layout: scene surfaces that enclose the viewer, such as walls and floor, represented as full, amodal extent (what the scene would look like w/o objects) - objects: a discrete set of objects factored into 3D shape (voxels) and pose (rotation and translation).</li>
</ul>
<h4 id="method-1">Method</h4>
<ul>
<li>Overall pipeline: One module to infer an amodal layout describing <em>the scene minus the object</em> and another object-centric module to map bounding boxes to object shape and pose.</li>
<li>Layout: - Treat layout as per pixel disparity estimation using skip-connected encoder-decoder architecture similar to [20] (FlowNet?) - Loss is l1 distance to ground truth. - But in this paper, the layout is supposed to be amodal (like objects do not exist), is amodal ground truth available at all? - While previous methods estimate vanishing-point-aligned 3D box which cannot generalize to non-box setup.</li>
<li>Object: - a anisotropic scaling <span class="math inline">\(c\)</span>, quaternion <span class="math inline">\(q\)</span> and translation <span class="math inline">\(t\)</span> in a canonical frame. - An encoder-decoder network is applied to a coarse image to estimate disparity map as layout. - Three types of features are stacked together to estimate shape and pose: - Feature map corresponding to the whole coarse image as context - ROI pooling feature from the finest image resolution - Features from fully connected layers atop bonding box coordinates - The above stacked feature is used to estimate - shape: via another 3D encoder-decoder structure to estimate occupancy grid (binary cross-entropy loss, need bootstrap training [9]) <span class="math inline">\(L_V\)</span> - scale: Euclidean distance of log scale <span class="math inline">\(L_c\)</span> - translation: Euclidean loss <span class="math inline">\(L_t\)</span> - rotation: quaternion, treat as classification, cross-entropy loss <span class="math inline">\(L_q\)</span></li>
<li>Total loss:</li>
</ul>
<p><span class="math display">\[
\sum_{b\in \mathcal{B}^+} (L_V+L_q+L_t+L_c-\ln(f)) + \sum_{b\in \mathcal{B}^-}\ln(1-f)
\]</span></p>
<p>where <span class="math inline">\(\mathcal{B}^+\)</span> denotes foreground proposals and <span class="math inline">\(f\)</span> is the probability of a proposal being on foreground.</p>
<div class="figure">
<img src="2018-04-05-16-20-00.png" />

</div>
<h4 id="experiments-1">Experiments</h4>
<ul>
<li>Train on SUNCG, which is synthetic and photo-realistic. Quantitative experiments are also performed on this dataset. - Restricted to a small set of interesting objects: bed, chair, desk, sofa, table and television.</li>
<li>Also test on NYU v2 to show qualitative results.</li>
<li>Results: - Rotation estimation as classification performs better than regression - Incorporating contextual feature (feature map from coarse image) is important for absolute scale and translation estimation, which <em>are hard to infer from a cropped image patch</em>.</li>
</ul>
<hr />
<h3 id="learning-category-specific-mesh-reconstruction-from-image-collections">Learning Category-Specific Mesh Reconstruction from Image Collections</h3>
<p><a href="https://arxiv.org/pdf/1803.07549.pdf" class="uri">https://arxiv.org/pdf/1803.07549.pdf</a></p>
<ul>
<li><p>Representation: - Shape as a 3D mesh <span class="math inline">\(M\)</span> comprised of faces <span class="math inline">\(F\)</span> and vertices <span class="math inline">\(V\in\mathbb{R}^{|V|\times 3}\)</span>, i.e., <span class="math inline">\(M=(F, V)\)</span>. - Connectivity of the mesh is pre-determined, i.e., <span class="math inline">\(F=\mathrm{contant}\)</span> - Category-wise mean shape defined as <span class="math inline">\(\bar V\)</span> - Instance-wise shape deformation defined as <span class="math inline">\(\Delta V\)</span> - Thus full shape is <span class="math inline">\(M=(F, \bar V+\Delta V)\)</span> - Camera pose defined as scale <span class="math inline">\(s\)</span>, translation <span class="math inline">\(t\)</span> and quaternion <span class="math inline">\(q\)</span>. - Keypoints: Defined as <span class="math inline">\(|K| \times |V|\)</span> association matrix <span class="math inline">\(A\)</span>, of which the k-th row denotes the probability distribution of a vertex being the k-th keypoint over all the vertices.</p></li>
<li>Training data <span class="math inline">\(\{I_i, S_i, x_i, \tilde \pi_i\}_{i=1}^N\)</span> - an RGB image <span class="math inline">\(I_i\)</span> of an object, - its 2D keypoints <span class="math inline">\(x_i\)</span> and - the foreground mask <span class="math inline">\(S_i\)</span> - also augmented with a weak projection <span class="math inline">\(\tilde \pi_i\)</span> which can be obtained via [11, 27]</li>
<li>Network <span class="math inline">\(f_\theta(\cdot)\)</span> predicts the following quantities: - Mesh deformation <span class="math inline">\(\Delta V\)</span> in a canonical frame. Full mesh which will be constructed by adding the deformation to the mean shape <span class="math inline">\((F, \bar V)\)</span>. - Pose <span class="math inline">\((s, t, q)\)</span> to bring the mesh from the canonical frame to camera frame. - Texture, which can be applied together with the mesh to generate a textured 3D object.</li>
<li><p>Loss: - Instance specific loss contains two terms, one to penalize keypoint projection matching and another for misalignment of segmentation mask - Keypoint: <span class="math inline">\(L_\text{reproj}=\sum_i{\| x_i - \tilde \pi_i(AV_i)\|_2}\)</span> - Mask: <span class="math inline">\(L_\text{mask}=\sum_i{\|S_i - \mathcal{R}(V_i, F, \tilde \pi_i)\|_2}\)</span>, where <span class="math inline">\(\mathcal{R}\)</span> is a differentiable <em>Neural Mesh Render</em> [28] - Regression loss on camera pose: <span class="math inline">\(L_\text{cam}\sum_i{\|\tilde \pi_i - \pi_i\|_2}\)</span> - Priors: - Surface smoothness: <span class="math inline">\(L_\text{smooth}\|LV\|_2\)</span>, where <span class="math inline">\(L\)</span> is the graph Laplacian operator. - Deformation regularization: the instance shape should not differ from the category mean too much, i.e., magnitude of instance deformation <span class="math inline">\(\Delta V\)</span> should be small, <span class="math inline">\(L_\text{def}=\|\Delta V\|_2\)</span> - Keypoint association: the association matrix <span class="math inline">\(A\)</span> should give peaked distribution -&gt; minimize the average entropy over all keypoints <span class="math inline">\(L_\text{vert2kp}=\frac{1}{|K|}\sum_{k, v}-A_{k, v}\log A_{k, v}\)</span> - Total loss <span class="math inline">\(L=L_\text{proj}+L_\text{mask}+L_\text{cam}+L_\text{smooth}+L_\text{def}+L_\text{vert2kp}\)</span>. - Symmetry constraints: imposed by the vertex connectivity <span class="math inline">\(F\)</span>, pre-defined. - Shape initialized as convex hull.</p>
<pre><code>![](2018-04-06-07-58-16.png)</code></pre></li>
<li><p>Texture learning and experiments: see paper.</p></li>
</ul>
<hr />
<h3 id="multi-view-consistency-as-supervisory-signal-for-learning-shape-and-pose-prediction">Multi-view Consistency as Supervisory Signal for Learning Shape and Pose Prediction</h3>
<p>CVPR 18.</p>
<p><a href="https://arxiv.org/pdf/1801.03910.pdf" class="uri">https://arxiv.org/pdf/1801.03910.pdf</a></p>
<ul>
<li>Multi-view supervision with <em>unknown</em> pose - Previous multi-view object reconstruction methods require pose supervision</li>
<li>Essentially the setup is the same as the joint depth and ego-motion prediction problem, which is usually tackled on KITTI dataset. The equivalence can be established by checking the following: - In this work, occupancy grid is predicted, which is equivalent to depth estimation in the SLAM setting - In this work, the camera pose between different views is unknown -- the same as SLAM where one predict the ego-motion from consecutive frames - Though with different representations, the loss might differ</li>
<li>The related work section is quite informative. Three categories of related methods are mentioned: 1. generative 3d modeling without 3d supervision 2. multi-view supervision for single-view depth prediction (two recent ones both in cvpr17:lr consistency, depth and ego-motion) 3. multi-view supervision for single-view reconstruction(PTN and some others)</li>
<li><p>Method: - two CNNs, one for shape <span class="math inline">\(f_s(\cdot)\)</span> and another for pose <span class="math inline">\(f_p(\cdot)\)</span> - per-pixel differentiable ray consistency:</p>
<pre><code>$$
L_p(\bar x, C; v_p) = \sum_{i=1}^{N} q_p(i) \psi_p(i)
$$
- where $\bar x$ is the predicted shape, $C$ is the camera pose, $v_p$ denotes the observation at pixel $p$. $\psi_p(i)$ denotes the cost for each event, determinted by $v_p$ and $q_p(i)$ indicates the event probability (the likelihood of the ray stopping at the i-th voxel in its path). The event probability is constructed using the probabilistic occupancy grid $\{x_p^i\}$. For details, see Appendix or the cvpr17 differentiable ray consistency paper.
- sampling occupancies along a ray: to make the ray consistency also differentiable w.r.t. camera pose, one can do the following:
    1.  Sample distances along the ray determined by the pixel coordinates of the observation.
    2.  Construct 3D points $\{x_c\}$ based on the distance samples in the the current camera frame.
    3.  Apply the camera pose (or its inverse, dependent on how camera pose is defined) to bring the 3D sample points to the canonical object frame to obtain $\{x_o\}$. (Note, in this paper, shape is defined in a canonical frame)
    4.  Obtain values at $\{x_c\}$ by trilinearly interpolating values at $\{x_o\}$ in the canonical object frame.</code></pre></li>
<li>Loss: <span class="math inline">\(L_\text{data}=\sum_{i=1}^N \sum_{u=1}^{N_i} \sum_{v=1}^{N_i} L(f_s(I_u^i), f_p(I_v^i); V_v^i)\)</span> where <span class="math inline">\(i\)</span> is instance index; <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> are viewpoint indices and <span class="math inline">\(V_v^i\)</span> is the observation of instance <span class="math inline">\(i\)</span> at viewpoint <span class="math inline">\(v\)</span>. - so essentially, for each training pair, shape is only predicted from <span class="math inline">\(I_u\)</span> and pose is only predicted from <span class="math inline">\(I_v\)</span>. But since all possible pairs are considered, the network should learn both shape and pose prediction from images at viewpoints. - to avoid local minima in pose prediction: - apply prior on pose (now the loss involves the difference of prediction to pre-set pose in addition) - classification instead of regression on pose</li>
<li>Parametrization: - rotation parametrized as azimuth and elevation</li>
<li><p>Main references: - [13] Unsupervised 3d shape induction from 2d views of multiple objects. 3DV, 2017. - [36] Rethinking reprojection: Closing the loop for pose-aware shape reconstruction from a single image. ICCV, 2017.</p></li>
</ul>
<hr />
<h3 id="neural-3d-mesh-renderer">Neural 3D Mesh Renderer</h3>
<p>CVPR 18.</p>
<p><a href="https://arxiv.org/pdf/1711.07566.pdf" class="uri">https://arxiv.org/pdf/1711.07566.pdf</a></p>
<ul>
<li>What is the most appropriate representation for the 3D world? - voxels: most widely used due to its regular data structure and suitable for 3D convolutions, resolution is limited due to high memory deman - point cloud: good scability, lack topology (no surface modeling, makes it hard to model texture) - polygon mesh: compactness (surfaces are explicitly modeled, no need for enormous amount of samples) and suitability for geometric transformation (only manipulation of vertices is involved)</li>
<li>Difficulty of using polygon mesh representation for learning: - Rasterization is not differentiable - Is this true? As long as visible vertices are projected to the image plane, other object properties can be obtained by running a per-pixel interpolation algorithm in barycentric coordinates defined by each triangle. Interpolation is, obviously, differentiable. However, the process of determining visibility is not differentiable.</li>
</ul>
<h4 id="method-2">Method</h4>
<ul>
<li>Representation: a set of vertices in object frame <span class="math inline">\(\{v_i^o\}_{i=1}^{N_v}\)</span>, where <span class="math inline">\(v_i^o \in \mathbb{R}^3\)</span>, and a set of faces <span class="math inline">\(\{f_i\}_{i=1}^{N_f}\)</span>, where <span class="math inline">\(f_i \in \mathbb{N}^3\)</span>. Vertices then can be transformed to screen space <span class="math inline">\(\{v_i^s\}\)</span>, where <span class="math inline">\(v_i^s \in \mathbb{R}^2\)</span> by Euclidean transformation and perspective projection.</li>
<li>The paper proposed a method to approximate the gradient <span class="math inline">\(\frac{\partial I}{\partial x}\)</span> by investigating two 1D cases. However, it seems this is just like the standard method: - Apply Gaussian kernels first to blur the sharp boundaries of rendering - Apply finite difference to compute the gradients - How different is this one from OpenDR?</li>
<li>Single image 3D reconstruction: - Base shape: isotropic sphere with 642 vertices - Vertex <span class="math inline">\(v_i+b_i+c\)</span> where <span class="math inline">\(v_i\)</span> is the base vertex, <span class="math inline">\(b_i\)</span> is local bias and <span class="math inline">\(c\)</span> is global bias - Vertices are constrained to move within their original quadrant - Loss contains two terms: one term to compare the rendered silhouette and ground truth silhouette, <span class="math inline">\(L_{sl}(x|\phi_i, s_i)=-\frac{|s_i \odot \hat s_i|_1}{|s_i + \hat s_i - s \odot \hat s_i|_1}\)</span> -- binary intersection over union; the other to regularize prediction by enforcing smoothness: <span class="math inline">\(L_{sm}=\sum_{\theta_i\in\mathcal E}(\cos\theta_i+1)^2\)</span> where <span class="math inline">\(\theta_i\)</span> is the angle between two faces which share the same edge <span class="math inline">\(i\)</span>.</li>
<li><p><strong>Need to understand the gradient approximation in details.</strong></p></li>
<li><p>Reference: - Early work use OpenGL in network training: 23 - Single image 3D reconstruction: - via depth estimation: 4, 25 - directly predict 3D shapes (voxel grids): 2, 5, 30, 31, 34 - Similar work: PTN, with voxel representation [36]</p></li>
</ul>
<hr />
<h3 id="multi-view-supervision-for-single-view-reconstruction-via-differentiable-ray-consistency">Multi-view supervision for single-view reconstruction via differentiable ray consistency</h3>
<p>CVPR 17</p>
<ul>
<li>Introduction &amp; Related work are useful, come back later</li>
<li>Representation
<ul>
<li>Shape Representation: 3D shape representation is parametrized as occupancy (misnomer, actually the probability of being empty) probabilities of cells in a discretized 3D voxel grid, denoted by the variable <span class="math inline">\(x\)</span>. Discretization of the 3D space not necessarily to be uniform -- as long as it's possible to trace rays across the voxel grid and compute intersections with cell boundaries.</li>
<li>Observation: treat various types of observations alike, including depth images, foreground masks, etc. Observation-camera pair <span class="math inline">\((O, C)\)</span> -- observation <span class="math inline">\(O\)</span> is measured by camera <span class="math inline">\(C\)</span>.</li>
</ul></li>
<li>View-consistency loss: <span class="math inline">\(L(x; (O, C))\)</span>. Given the camera intrinsics, we can convert the constraint provided by each pixel of observation <span class="math inline">\(O\)</span> to a constraint of a specific ray <span class="math inline">\(r\)</span>, which passes through the corresponding pixel. <span class="math inline">\(L(x; (O, C))=\sum_r L_r(x)\)</span>. <em>The task for formulating the view consistency loss is simplified to defining a differentiable ray consistency loss <span class="math inline">\(L_r(x)\)</span>.</em></li>
<li>Ray-tracing in a probabilistic occupancy grid:
<ul>
<li>Motivation: <em>The probabilistic occupancy model induces a distribution of events that can occur to ray <span class="math inline">\(r\)</span> and we can define <span class="math inline">\(L_r(x)\)</span> by seeing the incompatibility of these events with the available observation <span class="math inline">\(o_r\)</span>.</em></li>
<li>Ray termination events: random variable <span class="math inline">\(z_r\)</span> denotes the voxel in which the ray terminates.</li>
<li><span class="math inline">\(z_r=i\)</span> iff the previous voxels in the path are all un-occupied and the <span class="math inline">\(i\)</span>-th voxel is occupied, probability distribution of <span class="math inline">\(z_r\)</span>:</li>
</ul>
<p><span class="math display">\[
p(z_r=i)=
\begin{cases}
(1-x_i^r)\prod_{j=1}^{i-1}x_j^r, &amp;i \le N_r\\
\prod_{j=1}^{N_r}x_j^r, &amp;i=N_r+1
\end{cases}
\]</span></p>
where event <span class="math inline">\(z_r=N_r+1\)</span> means the ray doesn't terminate -- ray goes into to empty space.</li>
<li>Per-ray consistency loss:
<ul>
<li>Given depth observations:</li>
</ul>
<p><span class="math display">\[
\psi_r^{\text{depth}}(i)=|d_i^r-d_r|
\]</span></p>
where <span class="math inline">\(d_i^r\)</span> is the dpeth computed from the grid index <span class="math inline">\(i\)</span>. Also we have an associated probability for <span class="math inline">\(p(z_r=i)\)</span> which is given above.
<ul>
<li>Given foreground mask:<span class="math inline">\(s_r\in \{0, 1\}\)</span> where 0 denotes object mask and 1 otherwise.</li>
</ul></li>
</ul>
<p><span class="math display">\[
\psi_r^{\text{mask}}(i)=
\begin{cases}
s_r, &amp;i \le N_r\\
1-s_r, &amp; i=N_r+1
\end{cases}
\]</span></p>
<ul>
<li><p>Ray consistency loss:</p>
<p><span class="math display">\[
\begin{aligned}
L_r(x) &amp;= \mathbb{E}_{z_r}[\psi_r(z_r)]\\
&amp;=\sum_{i=1}^{N_r+1}\psi_r(i)p(z_r=i)
\end{aligned}
\]</span></p></li>
<li>Derivatives of loss w.r.t. CNN predictions <span class="math inline">\(x\)</span>: see appendix.</li>
<li>Further reading: [16, 26, 31] detailed reconstruction.
<ul>
<li>Hierarchical Surface Prediction for 3D Object Reconstruction</li>
<li>OctNetFusion</li>
</ul></li>
<li><p>TODO: study the code. (in lua, should be fun.)</p></li>
</ul>
<hr />
<h3 id="unsupervised-learning-of-depth-and-ego-motion-from-video">Unsupervised Learning of Depth and Ego-Motion from Video</h3>
<p>CVPR 17</p>
<ul>
<li>[10, 14, 16] seem quite related. Differences: previous work requires relative camera motion during training, while this work produces camera motion as one of the outputs.
<ul>
<li>[10] DeepStereo: Learning to predict new views from the world's imagery.</li>
<li>[14] Unsupervised CNN for single view depth estimation: Geometry to the rescue.</li>
<li>[16] Unsupervised monocular depth estimation with left-right consistency.</li>
</ul></li>
<li>[14] requires pose supervision; [7] Eigen's single-view depth estimation paper: requires full depth supervision.</li>
<li>concurrent work: SfM-Net: Learning of structure and motion from video. Differences: SfM-Net explicitly solves for object motion while this work discounts regions undergoing motion, occlusion and other factors via <em>explainability mask</em>.</li>
<li>While a single-view depth CNN and a camera pose estimation CNN are jointly trained from unlabeled video sequences, the resulting depth model and the pose estimation model can be used independently during inference.</li>
<li>Training samples: short image sequences of scenes captured by a moving camera; scene structure should be mostly rigid and static.</li>
<li>View synthesis objective:</li>
</ul>
<p><span class="math display">\[
  L_{vs}=\sum_s\sum_p|I_t(p)-\hat I_s(p)|
\]</span></p>
<pre><code>where $\hat I_s$ is the source view $I_s$ warped to the target coordinate frame based on predicted depth image $\hat D_t$ and camera motion $\hat T_{t\rightarrow s}$. Differentiable rendering module [8].</code></pre>
<ul>
<li>Warping from target pixel location <span class="math inline">\(p_t\)</span> to source view <span class="math inline">\(p_s\)</span>:</li>
</ul>
<p><span class="math display">\[
p_s \sim K\hat T_{t\rightarrow s}\hat D_t(p_t) K^{-1}p_t
\]</span></p>
<p>where <span class="math inline">\(K\)</span> is camera intrinsic matrix.</p>
<ul>
<li>To handle cases where the assumptions of static scene, no-occlusion and Lambertian surfaces dont hold, train a <em>explainability prediction</em> network to mask out target-source pairs by the per-pixel soft masks <span class="math inline">\(\hat E_s\)</span>. Weighted view synthesis loss:</li>
</ul>
<p><span class="math display">\[
L_{vs}=\sum_{&lt;I_1, \cdots, I_N&gt;} \sum_p \hat E_s(p)|I_t(p)-\hat I_s(p)|
\]</span></p>
<p>where trivial solution exists which is <span class="math inline">\(\hat E_s\)</span> always being zero. Introduce regularization term <span class="math inline">\(L_{reg}(\hat E_s)\)</span> that <em>encourages nonzero predictions by minimizing the cross-entropy loss with constant label 1 at each pixel location</em>.</p>
<ul>
<li>Gradient locality: gradients are mainly derived from the pixel intensity difference, which will be problematic if
<ul>
<li>the correct projection is located in a low-texture region</li>
<li>the current estimate is far away from the correct one</li>
</ul></li>
<li>Two ways to handle gradient locality:
<ul>
<li>use convolutional encoder-decoder archticture with a small bottleneck for the depth network that implicitly constrains the output to be globally smooth and facilitates gradients to propagate from meaningful regions to nearby regions.</li>
<li>explicit multi-scale and smoothness loss [14, 16] that allows gradients to be derived</li>
</ul></li>
<li>Single-view depth prediction as a module in the whole system is adopted from DispNet[35]. Also look at [47] DeMoN.</li>
<li><p><strong>Camera motion estimation outperforms ORB-SLAM running on short sequences without relocalization &amp; loop closure, especially in the case of forward motion -- quite intrigering result!!!</strong></p></li>
<li><p><a href="https://github.com/ClementPinard/SfmLearner-Pytorch">PyTorch implementation</a></p></li>
</ul>
<hr />
<h3 id="perspective-transformer-nets-learning-single-view-3d-object-reconstruction-without-3d-supervision">Perspective Transformer Nets: Learning Single-View 3D Object Reconstruction without 3D Supervision</h3>
<p>Simple approach, well written.</p>
<p>NIPS 2016</p>
<p><a href="https://arxiv.org/abs/1612.00814" class="uri">https://arxiv.org/abs/1612.00814</a></p>
<ul>
<li>The paper showed that it's feasible to train a single network for multi-class 3D object volumetric reconstruction without using ground truth 3D volumetric data for training</li>
<li>Image of an object is the entanglement of its intrinsic (shape, color, texture, etc.) and extrinsic (viewpoint, illumination, etc.) properties. The purpose of understanding 3D objects is to disentangle intrinsic and extrinsic properties from a single image.</li>
</ul>
<h4 id="method-3">Method</h4>
<ul>
<li>Notation: Image <span class="math inline">\(I^{(k)}\)</span> at k-th viewpoint is <span class="math inline">\(I^{(k)}=P(X; \alpha^{(k)})\)</span> where <span class="math inline">\(\alpha^{(k)}\)</span> encodes the viewpoint parameters. Use voxel grid as 3D representation: state of i-th voxel grid <span class="math inline">\(V_i \in \{0, 2\}\)</span>. 2D silhouette <span class="math inline">\(S^{(k)}\)</span> can be obtained via segmentation in practice, but in is trivial to obtain in this paper due to the white background assumption. 3D-2D projection: <span class="math inline">\(S^{(k)}=P(V; \alpha^{(k)})\)</span>.</li>
<li>Model: Encoder-decoder network to predict the state of voxels: <span class="math inline">\(\hat V = f(I^{(k)})\)</span>. Encoder <span class="math inline">\(h(\cdot)\)</span> learns a viewpoint-invariant representation <span class="math inline">\(h(I^{(k)}\)</span> and decoder generates the volume <span class="math inline">\(\hat V=g(h(I^{(k)}))\)</span>.</li>
<li>Perspective Transformation: One wants to compute the target volume <span class="math inline">\(U=\{x_i^t, y_i^t, z_i^t\}_{i=1}^{H&#39;\times W&#39;\times D&#39;}\)</span> given the source volume <span class="math inline">\(V=\{x_i^s, y_i^s, z_i^s\}_{i=1}^{H\times W\times D}\)</span> and the source to target transformation <span class="math inline">\(\Theta\doteq K[R | t]\)</span>. Given a target location <span class="math inline">\(X_i^t\)</span> defined in 3D discrete grid, we can first find its source location via <span class="math inline">\(X^s=\Theta^{-1}X^t\)</span> which has 3 real valued components and compute its value via linear interpolation:</li>
</ul>
<p><span class="math display">\[
    U_i=\sum_{n}^H \sum_{m}^W \sum_{l}^D V_{nml} \max(0, 1-|x_i^s-m|) \max(0, 1-|y_i^s-n|) \max(0, 1-|z_i^s-l|).
\]</span> - Basically, the interpolated value <span class="math inline">\(U_i\)</span> at <span class="math inline">\(X_i^t\)</span> is determined by the 8 vertices of the enclosing cube of its source location <span class="math inline">\(X_i^s\)</span>. - Projection is done by taking the maximum along the viewing direction. Since as long as there is at least one non-zero voxel intersecting the viewing direction, the pixel on image plane should be hit.</p>
<p><span class="math display">\[
S_{n&#39;m&#39;}=\max_{l&#39;} U_{n&#39;m&#39;l&#39;}
\]</span> where</p>
<ul>
<li>Loss: - If 3D volumetric ground truth <span class="math inline">\(V\)</span> is available: <span class="math inline">\(L_{vol}(I^{(k)})=\| f(I^{(k)}) - V\|_2^2\)</span> - Otherwise, use 2D silhouette as supervision:</li>
</ul>
<p><span class="math display">\[
    L_{proj}(I^{(k)})=\sum_{j=1}^n L_{proj}^{(j)}\big( I^{(k)}; S^{(j)}, \alpha^{(j)}\big)=\frac{1}{n}\sum_{j=1}^n \| P(f(I^{(k)}); \alpha^{(ij)}) - S^{(j)}\|_2^2
\]</span></p>
<div class="figure">
<img src="2018-04-15-17-29-18.png" />

</div>
<ul>
<li>Two-stage training (to overcome training difficulty): train the decoder first (this can be done w/o PTN and solely on 2D images) and then train the decoder with PTN. Essentially, the decode should learn a viewpoint-invariant representation of objects, which can be learned via normal object recognition scheme.</li>
</ul>
<hr />
<h3 id="unsupervised-learning-of-3d-structure-from-images">Unsupervised Learning of 3D Structure from Images</h3>
<p>NIPS 16</p>
<p><a href="https://arxiv.org/pdf/1607.00662.pdf" class="uri">https://arxiv.org/pdf/1607.00662.pdf</a></p>
<ul>
<li>Related work:
<ul>
<li>traditional vision as inverse graphics [24,21,23] (mostly from Josh Tenebaum's group) analysis-by-syntehesis [28,34,19,35] rely on heavily engineered visual features with simplified inference objectives</li>
<li>some limitations are addressed by recent work [19,5,4,37] by learning part of the encoding-decoding pipeline</li>
<li>this paper is different from previous ones in the sense that:
<ul>
<li>first 3D representation learned <em>end-to-end</em> directly from 2D images <em>without supervision</em></li>
</ul></li>
</ul></li>
</ul>
<h4 id="conditional-generative-models">Conditional Generative Models</h4>
<ul>
<li>Notations:
<ul>
<li>Observed volume or image <span class="math inline">\(x\)</span></li>
<li>A context <span class="math inline">\(c\)</span> which is available in both training and inference time. 3 forms of conditions are considered in experiments:
<ul>
<li>nothing</li>
<li>an object class label or</li>
<li>one or more views of the scene from different viewpoints</li>
</ul></li>
<li>The objective is to infer a 3D representation <span class="math inline">\(h\)</span> either in the form of a 3D volume or a mesh, which is enabled by modeling the latent manifold of object shapes and poses via the low-dimensional code <span class="math inline">\(z\)</span>.</li>
</ul></li>
<li>Generative models with latent variables describe probability densities <span class="math inline">\(p(x)\)</span> over datapoints <span class="math inline">\(x\)</span> implicitly (why implicitly? since we can sample from it but do not know the exact form of the distribution) through a marginalization of the set of latent variables <span class="math inline">\(z\)</span>, <span class="math inline">\(p(x)=\int p_\theta(x|z) p(x) dz\)</span>.</li>
<li>The marginal likelihood <span class="math inline">\(p(x)\)</span> is intractable and approximations are resorted, such as variational inference, where the marginal likelihood <span class="math inline">\(p(x)\)</span> is bound by <span class="math inline">\(\mathcal{F}=\mathbb{E}_{q(z|x)}[\log p_\theta(x|z)] - KL[q_\phi(z|x) \| p(z)]\)</span> where the true posterior is approximated by a parametric family of posterior <span class="math inline">\(q_\phi(z|x)\)</span> with parameters <span class="math inline">\(\phi\)</span>. Variational parameters <span class="math inline">\(\phi\)</span> and model parameters <span class="math inline">\(\theta\)</span> are found jointly.</li>
<li>We can think of the generative model as a decoder of the latent variables, and the inference network as an encoder of the observed data into the latent representation.</li>
<li>Gradients of <span class="math inline">\(\mathcal{F}\)</span> can be estimated using reparametrization trick. [14,18] Kingma</li>
<li>Need to read the DRAW paper first to understand this one. Seems in terms of architecture, this paper follows the DRAW paper and only briefly touches it here.
<ul>
<li>FIXME: <strong><em>DRAW: A recurrent neural network for image generation. ICML, 2015.</em></strong></li>
</ul></li>
</ul>
<hr />
<h3 id="octree-generation-networks-efficient-convolutional-architectures-for-high-resolution-3d-outputs">Octree generation networks: Efficient convolutional architectures for high-resolution 3d outputs</h3>
<p>ICCV 2017</p>
<p><a href="https://arxiv.org/pdf/1703.09438.pdf" class="uri">https://arxiv.org/pdf/1703.09438.pdf</a></p>
<ul>
<li>Related work: - supervised [15,6,8,18] and - unsupervised [38,13,20] 3D object reconstruction - probabilistic generative models [37,36,30] - segmentation [3,5] - shape deformation [40] - inspired by sparse convolution papers [16,17] - similar idea: OctNet [29]</li>
</ul>
<h4 id="octree-representation">Octree Representation</h4>
<ul>
<li>Instead of the naive implementation of pointers-to-childen to maintain the recursive structure, use hash tables to achieve constant query time.</li>
<li>An octree cell with spatial coordinates <span class="math inline">\(x\in\mathbb{R}^3\)</span> at level <span class="math inline">\(l\)</span> is represented as an index-value pair <span class="math inline">\((m, v)\)</span> where <span class="math inline">\(v\)</span> can be any kind of discrete or continuous signal and <span class="math inline">\(m\)</span> is computed from <span class="math inline">\((x, l)\)</span> using <em>Z-order curves</em>[14]: <span class="math inline">\(m=\mathcal{Z}(x, l)\)</span>, which is computationaly cheap due to its use of bit shifts.</li>
<li>An octree <span class="math inline">\(O\)</span> is a set of all pairs <span class="math inline">\(O\doteq\{(m, v)\}\)</span>. Storing this set as a hash table allows for constant-time random access.</li>
<li>Compare values at the same spatial location in two octree trees <span class="math inline">\(O^1\)</span> and <span class="math inline">\(O^2\)</span>. Say we want to compare the value at <span class="math inline">\((x, l)\)</span> in <span class="math inline">\(O^1\)</span> which has index <span class="math inline">\(Z(x, l)\)</span> to the value at same location in <span class="math inline">\(O^2\)</span>. If there exists a cell <span class="math inline">\((x, k)\)</span> with <span class="math inline">\(k &lt;= l\)</span> in <span class="math inline">\(O^2\)</span>, we obtain the value immediately, since in <span class="math inline">\(O^2\)</span>, the whole block, which contains <span class="math inline">\((x, l)\)</span>, at level <span class="math inline">\(k\)</span>, which is coarser than <span class="math inline">\(l\)</span>, has the same value. If <span class="math inline">\(l &gt; k\)</span>, we cannot obtain the value at <span class="math inline">\((x, l)\)</span> in <span class="math inline">\(O^2\)</span> unless we further subdivide the block <span class="math inline">\((x, l)\)</span> in <span class="math inline">\(O^1\)</span>. A function <span class="math inline">\(f\)</span> is introduced for querying the signal value of an arbitrary cell with index <span class="math inline">\(m=\mathcal{Z}(x, l)\)</span> from octree <span class="math inline">\(O\)</span>:</li>
</ul>
<p><span class="math display">\[
f(m, O) = 
\begin{cases}
v, \quad \text{if} k \le l: (\mathcal{Z}(x, k), v) \in O\\
\emptyset, \quad \text{otherwise}
\end{cases}
\]</span></p>
<ul>
<li>Feature maps are representeted as a set of features indexed by the hash function. Each cell has 3 possible states: empty, mixed and filled, each means the cell is not used at all, the cell should be subdivided and the whole cell is filled. This is mostly useful in training, where the predicted octree is compared to the ground truth octree: those cells whose divisions are consistent in two and filled in ground truth are marked as filled, those empty in ground truth are marked as emtpy. Both states are back-propagated such that the division scheme can be learned from the ground truth (actually no need for furtuer division, but label the cell correctly). For those cells which have different granualities of division in prediction and ground truth are marked as mixed. The mixed state is also back-propagated such the proper division (here, this is real division instead of changing states) can be learned.</li>
<li>For loss and more details, come back to this paper if needed. Authors provide implementation integrated to Caffe.</li>
<li>This is really fundamental data structure and low level implementation.</li>
</ul>
<hr />
<h3 id="learning-a-probabilistic-latent-space-of-object-shapes-via-3d-generative-adversarial-modeling">Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling</h3>
<p><a href="https://arxiv.org/pdf/1610.07584.pdf" class="uri">https://arxiv.org/pdf/1610.07584.pdf</a></p>
<p>NIPS 16</p>
<ul>
<li>In terms of 3D voxel decoder, similar to DC-GAN (Radford, 2016).</li>
<li>In terms of 3D-VAE-GAN, similar to VAE-GAN (Larsen, 2016).</li>
<li><p>Losses: - <span class="math inline">\(L_\text{3D-GAN}=\log D(x) + \log (1-D(G(z)))\)</span>, where <span class="math inline">\(x\)</span> is a real image, <span class="math inline">\(z\)</span> is the latent representation, <span class="math inline">\(G\)</span> is the convolutional generator which generates the voxel-based shape, <span class="math inline">\(D\)</span> is the discriminator which should give high score to fake samples and low score to real samples. Intuitively, generator <span class="math inline">\(G\)</span> tries to generate a as realistic as possible sample to confuse the discriminator and the discriminator tries to distinguish fake from real. So to minimize this loss, the <span class="math inline">\(D\)</span> is trained in such a way that it scores lower to real samples. For the second log to be lower, D(G(z)) should be higher meaning D should distinguish fake samples generated by G which tries to make the fake samples realistic. - <span class="math inline">\(L_\text{KL}=D_\text{KL}\big(q(z|y)||p(z)\big)\)</span> uses a distribution family <span class="math inline">\(q(z|y)\)</span> to approximate the true marginal <span class="math inline">\(p(z)\)</span> and we want to minimze their KL divergence. - <span class="math inline">\(L_\text{recon}=\|G(E(y)) - x \|_2\)</span> where <span class="math inline">\(y\)</span> is the input image and <span class="math inline">\(E\)</span> encodes the image into a latent representation.</p></li>
<li><p>Main references: - FIXME: (DC-GAN) Unsupervised representation learning with deep convolutional generative adversarial networks. ICLR, 2016. - FIXME: (VAE-GAN) Autoencoding beyond pixels using a learned similarity metric. ICML, 2016.</p></li>
</ul>
<hr />
<h3 id="marrnet-3d-shape-reconstruction-via-2.5d-sketches">MarrNet: 3D Shape Reconstruction via 2.5D Sketches</h3>
<p><a href="http://marrnet.csail.mit.edu/papers/marrnet_nips.pdf" class="uri">http://marrnet.csail.mit.edu/papers/marrnet_nips.pdf</a></p>
<p>NIPS 17</p>
<ul>
<li>Reconstruct the shape of an object from a single image with two steps: 1. Compute a 2.5D sketch including depth map, normal map and silhouette from the image 2. Reconstruct the shape from the 2.5D sketch as voxel grid.</li>
<li>Advantages of the two-step approach compared to direct method (predict shape directly from image): 1. 2.5D sketches are easier to be recovered from a 2D image compared to 3D shapes 2. 3D reconstruction from 2.5D sketches can be learned purely from synthetic data. Since rendering 2.5D sketches wihtout modeling object appearance variations including lighting, texture, etc., is much easier, which also <em>relieves the domain adaption problem</em>. 3. A differentiable projective functions from 3D shape to 2.5D sketches is proposed so that the system can be trained end-to-end. (similar to DRC paper) <img src="marrnet_net.png" /></li>
<li>Reprojection consistency: - Notations: Random variable <span class="math inline">\(v_{x,y,z} \in [0,1]\)</span> denotes the probability of voxel at <span class="math inline">\((x,y,z)\)</span> being occupied. <span class="math inline">\(d_{x,y}\)</span> denotes the predicted depth at position <span class="math inline">\((x,y)\)</span> and <span class="math inline">\(n_{x,y}=(n_a,n_b,n_c)\)</span> the predicted surface normal. Assume orthographic projection (compared to perspective projection, no need to scale x and y by depth z). - Depth consistency: Given a predicted depth <span class="math inline">\(d_{x,y}\)</span> at pixel coordinates <span class="math inline">\((x,y)\)</span>, we can trace the viewing ray and all the voxels along the ray should NOT be occupied, otherwise they should be penalized. For those voxels behind <span class="math inline">\(d_{x,y}\)</span>, simply ignore them since they are invisible to which no assumptions can be made.</li>
</ul>
<p><span class="math display">\[
L_\text{depth}=
\begin{cases}
v_{x,y,z}^2, \quad z&lt;d_{x,y}\\
(1-v_{x,y,z})^2, \quad z=d_{x,y}\\
0, \quad z&gt;d_{x,y}
\end{cases}
\]</span> - Surface normal consistency: For a predicted surface normal <span class="math inline">\(n_{x,y}=(n_a, n_b, n_c)\)</span> to be meaningful, its neighboring voxels should sit on the same plane. We can simply check nearby voxels along the directions orthogonal to the normal, which are <span class="math inline">\(n&#39;_x=(0, -1, n_b/n_c)\)</span> and <span class="math inline">\(n&#39;_y=(-1, 0, n_a/n_c)\)</span>. The normal consistency loss is then</p>
<p><span class="math display">\[
L_\text{normal}(x,y,z)=(1-v_{(x,y,z)+n&#39;_x})^2 + (1-v_{(x,y,z)-n&#39;_x})^2 + (1-v_{(x,y,z)+n&#39;_y})^2 + (1-v_{(x,y,z)-n&#39;_y})^2
\]</span> - For the gradients of depth and surface normal consistency, see the paper. <img src="marrnet_consistency.png" /></p>
<ul>
<li>Training: 2-stage scheme: 1. the 2.5D sketch estimator and 3D shape predictor are trained separately on synthetic images with ground truth normal/depth/silhouette and voxels respectively. Cross-entropy loss is used for training of the shape predictor. 2. the network is fine-tuned on real images.</li>
<li>Comments: The intuition is really good -- training a system to directly predict 3D shapes from RGB images on synthetic data is challenging due to the complex image formation process where one has to model such that all the nuisance variability present in real images are also present in synthetic images. By the two-step formulation, one can rely on RGB-D data for training the 2.5D sketch predictor and synthetic data for training the 2.5D to 3D predictor, where one only needs to model geometric information, i.e., depth, normal and silhouette which are very easy to obtain.</li>
<li>Main references: - (3d shape encoder) Learning a predicatable and generative vector representation for objects. ECCV, 2016. - (3d shape decoder) 3D-VAE-GAN</li>
</ul>
<hr />
<h3 id="pix3d-dataset-and-methods-for-single-image-3d-shape-modeling">Pix3D: Dataset and Methods for Single-Image 3D Shape Modeling</h3>
<p><a href="https://arxiv.org/pdf/1804.04610.pdf" class="uri">https://arxiv.org/pdf/1804.04610.pdf</a></p>
<p>CVPR 18</p>
<ul>
<li>Pix3D: a large-scale benchmark for diverse image-shape pairs with pixel-level 2D-3D alignment - 395 3D shapes of - 9 object categories - 10069 image-shape pairs with precise 3D annotation: pixel-level alignment between shapes and their 2D silhouettes</li>
<li>problems of existing 2D-3D datasets: - ShapeNet: purely synthetic, no real imagery - Pascal 3D and ObjectNet3D: rough image-shape alignment, with inexact shape model - IKEA: high-quality 3D alignment, scale too small</li>
<li>steps to construct the dataset 1. collect image-shape pairs by web crawling and performing 3D scans 2. collect 2D keypoint annotations on Amazon Mechanical Turk with which 3D poses are optimized to align shapes with image silhouettes 3. filter out image-shape pairs with pool alignment and collect attributes (truncation, occlusion) for each instance also via crowdsourcing</li>
<li>calibrate widely used metrics for shape alignment: - IoU (Intersection over Union) - CD (Chamfer Distance): <span class="math inline">\(CD(\Omega_1, \Omega_2)=\frac{1}{|\Omega_1|}\sum_{x \in \Omega_1} \min_{y \in \Omega_2}\| x-y\|_2 + \frac{1}{|\Omega_2|}\sum_{y \in \Omega_2}\min_{x\in\Omega_1}\|x-y\|_2\)</span> - EMD (Earth Mover's Distance [12]): <span class="math inline">\(EMD(\Omega_1, \Omega_2)=\frac{1}{|\Omega_1|}\min_{\phi:\Omega_1\rightarrow\Omega_2}\|x-\phi(x)\|_2\)</span>. Expensive to compute and use the approximation [3] - Key findings: CD and EMD correlates better with human perception. See Table 1 and 2 where alignment with high IoU is not accepted by humans as good alignment.</li>
<li>Comparison: 3DR2N2, Differentialble Ray Consistencyt (Berkley CVPR17), Jiajun NIPS15 - Their approach: extension of MarrNet [61] - MarrNet for 2.5D sketch prediction as encoder: depth, normal and silhouette prediction - Attach decoder to MarrNet and predict: voxel grid of objects and viewpoints. - Training: - render objects from ShapeNet, 3 types of backgrounds: 1/3 white, 1/3 high-dynamic-range with illumination channels, 1/3 on randomly sampled images from SUN - train MarrNet first and then train MarrNet and the decoders jointly</li>
<li>Main reference: - FIXME: 61 <strong>MarrNet: 3D Shape Reconstruction via 2.5D Sketches, NIPS 17.</strong></li>
<li>Misc Notes: For detailed network architecture and qualitative results, see the SupMat.</li>
</ul>
<div class="figure">
<img src="pix3d_net.png" />

</div>
<hr />
<h3 id="pixels-voxels-and-views-a-study-of-shape-representations-for-single-view-3d-object-shape-prediction">Pixels, voxels, and views: A study of shape representations for single view 3D object shape prediction</h3>
<p><a href="http://www.ics.uci.edu/~fowlkes/papers/sfh-shapecompletion.pdf" class="uri">http://www.ics.uci.edu/~fowlkes/papers/sfh-shapecompletion.pdf</a></p>
<p>CVPR 18</p>
<ul>
<li>Compare <em>surface</em> and <em>volumetric</em> 3D object representations in <em>view-centric</em> and <em>object-centric</em> frames</li>
<li>Design a network to predict depth from either single depth or RGB image and adapt the network to produce surface/volumetric representations</li>
<li>Key findings: 1. surface-based methods outperform voxel representations for objects from <em>novel</em> classes and produce higher resolution outputs 2. using view-centric coordinates is advantageous for novel objects (makes sense since in view-centric coordinates, reconstruction is view-specific and there is no need to learn viewpoint invariant representation which requires the object being seen before) 3. while object-centric representations are better for more familiar objects (for familiar objects, the network should be able to learn a viewpoint invariant representation and predict class-specific shapes in a canonical frame where prior about the shape learned from shapes belonging to the same class can be used) 4. the coordinate frame significantly affects the shape representation learned: 5. object-centric representation places more importance on implicitly recognizing the object category (this observation is consistent with 3) 6. view-centric representations have less dependence on category recognition</li>
<li>Network architecture: - Input can be either a depth/silhouette pair or a single RGB imageo - The network follows a typical encoder-decoder structure. For the depth/silhouette type inputs, each stream goes into a decoder (<span class="math inline">\(E_d\)</span> and <span class="math inline">\(E_s\)</span>). The two feature maps are concatenated and fed to a third decoder <span class="math inline">\(E_h\)</span> which produces a feature map <span class="math inline">\(h\)</span> shared by a collection of decoders, each of which consists of a FC + UpConv structure, producing a silhouette and two (font and back) depth images at different viewpoints. - For multi-surface representation, Floating Scale surface Reconstruction (FSSR) [6] is appied to fit surfaces to the predicted depth map from different viewpoints. - For voxel representation, the decoders above are replaced by several 3D upconvolutional layers such that the feature map <span class="math inline">\(h\)</span> is decoded into a voxel representation.</li>
<li>FIXME: There are some questions we need to ask ourselves: 1. Can we directly predict parameters of a parametrized shape instead of predicting multi-view depth first? 2. If we are given multiple images from different viewpoints, can we directly estimate parametrized shapes?</li>
<li>Main references: - [6] Floating Scale Surface Reconstruction, TOG, 2014 - [13] Completing 3D object shape from on depth image. CVPR 2015.</li>
</ul>
<p><img src="pixels_voxels_views_representation.png" alt="representations" /> <img src="pixels_voxels_views_net.png" alt="architecture" /></p>
<hr />
<h3 id="im2struct-recovering-3d-shape-structure-from-a-single-rgb-image">Im2Struct: Recovering 3D Shape Structure from a Single RGB Image</h3>
<p><a href="https://kevinkaixu.net/papers/niu_cvpr18_im2struct.pdf" class="uri">https://kevinkaixu.net/papers/niu_cvpr18_im2struct.pdf</a></p>
<p>CVPR 18</p>
<ul>
<li>Recover 3D shape structures from single RGB image where <em>structure refers to shape parts represented by cuboids and part relations encompassing connectivity and symmetry</em></li>
<li>Encoder-deconder structure where: - the encoder is a multi-scale convolutional network - trained with the task of shape contour estimation (essentially forground/background segmentation in the setting of this work where objects have non-homogeneous but simple background) h - the decoder fuses 1. the features of the encoder and 2. the original image - the two networks are trained jointly using 1. contour-mask and 2. cuboid-structure pairs which are generated by rendering repository of 3D CAD models coming with <em>part segmentation</em>.</li>
<li>Structure Masking Network: - Inspired by [12] multi-scale network for detailed depth estimation - the first scale captures the information of the whole image - the second scale produces a detailed mask map at a quater of the input resolution - feature maps from the first scale are fed to the second scale via skip connections</li>
<li>Structure Recovery Network: - <strong>Feature fusion</strong> from two channels: 1. the feature map of the strcutre masking network (factor out nuisance variability, such as background clutters, texture and illumination changes) 2. VGG feature from original image (provides complimentary information missing in mask) - <strong>Structure decoding</strong> vai recursive neural network (RvNN) as <em>box structure decoder</em> as [13] - RvNN: starting from a root node, recursively decodes its into a hierarchy of features until reaching the leaf nodes which each can be further decoded into a vector of box parameters - 3 types of node: 1. leaf node (box decoder) 2. adjacency node (adjacency decoder) 3. symmetry node (symmetry decoder) - The latter two serve as internal notes indicating part relations. Each type of nodes requires a decoder (small network) to estimate parameters.</li>
</ul>
<p><img src="im2struct_nets.png" /> <img src="im2struct_decoder.png" /></p>
<ul>
<li>Main references:
<ul>
<li>[12]for the 2 scale structure mask network</li>
<li>FIXME [13]box structure decoding: Grass: Generative recursive autoencoders for shape structures.</li>
<li>[22]jiajun, nips16: Learning a probabilistic latent space of object shapes via 3D generative-adversarial modeling.</li>
</ul></li>
</ul>
<hr />
<h3 id="attentional-shapecontextnet-for-point-cloud-recognition">Attentional ShapeContextNet for Point Cloud Recognition</h3>
<p><a href="http://vcl.ucsd.edu/~sxie/pdf/cvpr_2018_shape_context.pdf" class="uri">http://vcl.ucsd.edu/~sxie/pdf/cvpr_2018_shape_context.pdf</a></p>
<p>CVPR 18</p>
<p>The goal is to handle the irregular domain of point cloud data. Unlike images and depth maps which are defined on regular grids, point cloud data have irregular domain and vanilla convolution cannot capture the context effectively. This paper borrows the idea from handcrafted shape context descriptors, and comes up with trainable Shape Context blocks which are then put together to build a ShapeContextNet (SCN) and Attentional ShapeContextNet (A-SCN), where ideas of self-attention are borrowed from NLP to build the latter.</p>
<p>Experiments show better performance than PointNet and comparable performance to the recent PointNoet++ paper in terms 2D point cloud classification (on MNIST), part segmentation (ShapeNet part) and scene segmentation (Standford Scene).</p>
<hr />
<h3 id="a-variational-u-net-for-conditional-appearance-and-shape-generation">A Variational U-Net for Conditional Appearance and Shape Generation</h3>
<p><a href="https://arxiv.org/pdf/1804.04694.pdf" class="uri">https://arxiv.org/pdf/1804.04694.pdf</a></p>
<p>CVPR 18</p>
<ul>
<li>Good references to GAN and VAE: - GAN: [1,8,10,27,38] - VAE: 16, 31 - Combination: [2,17]</li>
<li>Other useful references: - perception similarity: [7] Photographic image synthesis with cascaded refinement networks. ICCV, 2017. - pix2pix: [12] Image-to-image translation iwth conditional adversarial networks. CVPR, 2016. - Autoencoding beyond pixels using a learned similarity metric. arXiv, 2015.</li>
<li>Misc: The goal is to disentagle geometry (shape, pose) and appearance(texture, color) - The paper proposed a variational U-Net for this purpose. - Need to review VAE and come back to understand Section 3.</li>
</ul>
<hr />
<h3 id="pointgrid-a-deep-network-for-3d-shape-understanding">** PointGrid: A Deep Network for 3D Shape Understanding</h3>
<p>&lt;&gt;</p>
<hr />
<h3 id="learning-to-estimate-3d-human-pose-and-shape-from-a-single-color-image">Learning to Estimate 3D Human Pose and Shape from a Single Color Image</h3>
<p>&lt;&gt;</p>
<hr />
<h3 id="d-object-detection-with-latent-support-surfaces">* 3D Object Detection with Latent Support Surfaces</h3>
<p><a href="http://cs.brown.edu/~ren/publications/cvpr2018/3dlatent_cvpr2018.pdf" class="uri">http://cs.brown.edu/~ren/publications/cvpr2018/3dlatent_cvpr2018.pdf</a></p>
<p>RGB-D and non-deep learning.</p>
<hr />
<h3 id="reconstructing-thin-structures-of-manifold-surfaces-by-integrating-spatial-curves">Reconstructing Thin Structures of Manifold Surfaces by Integrating Spatial Curves</h3>
<hr />
<h3 id="learning-less-is-more---6d-camera-localization-via-3d-surface-regression">Learning Less is More - 6D Camera Localization via 3D Surface Regression</h3>
<hr />
<h3 id="d-registration-of-curves-and-surfaces-using-local-differential-information">3D Registration of Curves and Surfaces using Local Differential Information</h3>
<hr />
<h3 id="geonet-geometric-neural-network-for-joint-depth-and-surface-normal-estimation">GeoNet: Geometric Neural Network for Joint Depth and Surface Normal Estimation</h3>
<p><a href="http://www.cs.toronto.edu/~rjliao/papers/CVPR_2018_GeoNet.pdf" class="uri">http://www.cs.toronto.edu/~rjliao/papers/CVPR_2018_GeoNet.pdf</a></p>
<p>CVPR 18</p>
<ul>
<li>GeoNet (Geometric Neural Network) - jointly predict depth and surface normal maps from a single image - built atop two-stream CNNs -- one stream for depth prediction and one for normal - <em>depth-to-normal</em> and <em>normal-to-depth</em> networks incorporate geometric relation between the two streams</li>
<li>Terminologies: - direct outputs from depth/normal networks are called initial depth/normal - final outputs after refinement are called refined depth/normal - normal computed from initial depth is called rough normal</li>
<li>Depth-to-normal network: given initial depth prediction, compute per pixel rough surface normal via the following steps: 1. convert the initial depth map to a point cloud 2. compute rough normal prediction as the direction along which the coordinates of 3D points have least variance (in the paper, a least square solution is proposed, which is equivalent to applying SVD to the covariance of 3D points and find the vector with least singular value) 3. refine the rough normal via a small network which consists of conv layers and skip connections from both the rough normal and initial normal prediction</li>
<li>Normal-to-depth network: given initial normal prediction we can constrain the depth in a neighborhood in a way such that: - along the direction of normal, the variance of the 3D points formed by the depth is minimized - such constraints are applied to each pixel and smoothed via a linear kernel <img src="geonet_modules.png" /></li>
<li>Loss: - <span class="math inline">\(L_\text{depth}=\frac{1}{M}\big(\sum_i\|z_i - z_i^\text{gt}\|_2^2 + \mu \sum_i \|\hat z_i - z_i^\text{gt}\|_2^2\big)\)</span> - <span class="math inline">\(L_\text{normal}=\frac{1}{M}\big(\sum_i\|n_i - n_i^\text{gt}\|_2^2 + \lambda \sum_i \|\hat n_i - n_i^\text{gt}\|_2^2\big)\)</span> - where quantities with hat is refined (final) prediction, with superscript gt is ground truth and without any decoration is rough prediction</li>
<li>Misc: - For ground truth depth with missing/invalid values, run inpainting method of [17] to fll in values - Colorization using optimization. ToG, 2014 - Ground truth normal can be obtained from GT depth by using method of [9] - Data-driven 3D primitives for single image understanding. ICCV, 2013. - Least Square of normal computation is numerically instable due to the matrix inversion step, which can be mitigated by setting small value for <span class="math inline">\(\lambda\)</span>.</li>
<li>Main reference: - prior art: [16] Deeper depth prediction with fully convolutional residual networks. 3DV, 2016. - use semantics in geometric estimation: [19] Single image depth estimation from predicted semantic labels. CVPR, 2010. - FIXME: most similar [32]: SURGE: Surface regularized geometry estimation from a single image. NIPS, 2016. <img src="geonet_net.png" /></li>
</ul>
<hr />
<h3 id="codeslam-----learning-a-compact-optimisable-representation-for-dense-visual-slam">CodeSLAM --- Learning a Compact, Optimisable Representation for Dense Visual SLAM</h3>
<p><a href="https://arxiv.org/pdf/1804.00874.pdf" class="uri">https://arxiv.org/pdf/1804.00874.pdf</a></p>
<p>CVPR 18</p>
<ul>
<li>Motivation - dense reconstruction cannot be fully probabilistic due to the large amount of parameters - correlation between depth reconstruction among nearby pixels is NOT modeled (in LSD-SLAM, per pixel Kalman filter is used to estimate semi-dense depth maps but correlation between pixels is not considered) - Also it's infeasible to jointly optimize motion and structure with such parametrization. - sparse reconstruction can be formulated in a fully probabilistic framework. In both filtering and graph optimization methods, joint posterior of motion and structure given all the observation can be maintained: - In filtering approach, the posterior is explicitly maintained by updating mean and covariance of a Gaussian distribution. - In graph optimization approach, one usually attain a point estimate by maximizing the posterior. - But sparse method only models a tiny portion of the underlying scene.</li>
<li><em>The conclusion that a dense representation of the environment requires a large number of parameters is NOT necessarily correct.</em> The geometry of natural scenes is not a random collection of <em>occupied and unoccupied</em> space but exhibits a high degree of order. - Locally in a depth map, nearby pixels should have similar depth values - More strongly, a high-level (semantic) understanding of the scene can help decompose the scene into - a set of semantic objects together with - some intrinsic parameters of each object (SLAM++ like representation) - Knowledge about primitive shapes such as planar surfaces can also be incorporated into SLAM. - Such representations are much more compact compared to voxel representation and they are more detailed compared to sparse representation.</li>
<li>Representation: A compact but dense representation of scene geometry which is - conditioned on the intensity data from a single image and - conditioning: image provides local details and the code supplies more global shape priors - in reality the condiitoning is realized by concatenating RGB image features from an image decoder and depth features from a depth decoder. - generated from a code consisting of a small number of parameters. - this is achieved by a decoder detailed in Fig. 3 of the paper.</li>
<li>In training time, two encoder-decoder networks based on the U-Net architecture are trained: - One is an image autoencoder whose decoder outputs uncertainty of per pixel depth prediction. - The other is a depth map autoencoder. For the depth encoder, image features from image decoder are concatenated to depth features with same spatial size and compressed. In the connection between encoder and decoder, there is a variational module. (see VAE paper) The decoder outputs per pixel mean depth. - Given the predicted uncertainty and mean, a cost involving log likelihood of depth is formulated. The paper didn't explicitly give the loss, I dont know why, but they referred to the NIPS 17 paper.</li>
<li>In inference, depth map is not available which is OK, since essentially what one needs is the function to map the code to depth map and the the code as well as the relative poses of images are jointly optimized in a second-order optimization framework just like what people do in traditional SfM.</li>
<li>The energy function in inference is essentially a function of the relative motion between two frames and the code (depth map in traditional formulation) and one obtains the solution by minimizing reconstruction error which is essentially the photometric difference of two images of which one warped to the other according to the relative pose and depth decoded from the code (thats why we only need the decoder in inference).</li>
<li>So why do they use VAE in the network? The code given by VAE hopefully disentangles essential factors which can be used to reconstruct the scene and by searching a solution in the code space (carried out by optimization in inference), one can find a plausible dense reconstruction of the scene.</li>
<li>Misc: Jacobians of the energy w.r.t. relative translation, rotation and the compact code, see the paper.</li>
<li>Main refereces: - FIXME: use uncertainty in learning: [12] What uncertainties do we need in bayesian deep learning for computer vision. NIPS 17. - the VAE paper</li>
</ul>
<hr />
<h3 id="deep-marching-cubes-learning-explicit-surface-representations">** Deep Marching Cubes: Learning Explicit Surface Representations</h3>
<hr />
<h3 id="end-to-end-recovery-of-human-shape-and-pose">** End-to-end recovery of Human Shape and Pose</h3>
<p>CVPR 18</p>
<p><a href="https://arxiv.org/pdf/1712.06584.pdf" class="uri">https://arxiv.org/pdf/1712.06584.pdf</a></p>
<hr />
<h3 id="planenet-piece-wise-planar-reconstruction-froma-single-rgb-image">**PlaneNet: Piece-wise Planar Reconstruction froma Single RGB Image</h3>
<hr />
<h3 id="image-collection-pop-up-3d-reconstruction-and-clustering-of-rigid-and-non-rigid-categories">**Image Collection Pop-up: 3D Reconstruction and Clustering of Rigid and Non-Rigid Categories</h3>
<hr />
<h3 id="extreme-3d-face-reconstruction-looking-past-occlusions">**Extreme 3D Face Reconstruction: Looking Past Occlusions</h3>
<h3 id="section"></h3>
<hr />
<h2 id="mapping-and-planning">Mapping and Planning</h2>
<h3 id="unifying-map-and-landmark-based-representations-for-visual-navigation">Unifying Map and Landmark Based Representations for Visual Navigation</h3>
<p><a href="https://arxiv.org/pdf/1712.08125.pdf" class="uri">https://arxiv.org/pdf/1712.08125.pdf</a></p>
<hr />
<h3 id="cognitive-mapping-and-planning-for-visual-navigation">Cognitive Mapping and Planning for Visual Navigation</h3>
<p><a href="https://arxiv.org/abs/1702.03920" class="uri">https://arxiv.org/abs/1702.03920</a></p>
<h2 id="slam-meets-deep-learning">SLAM Meets Deep Learning</h2>
<h3 id="unsupervised-learning-of-depth-and-ego-motion-from-monocular-video-using-3d-geometric-constraints">Unsupervised Learning of Depth and Ego-Motion from Monocular Video Using 3D Geometric Constraints</h3>
<p>CVPR 18</p>
<p><a href="https://arxiv.org/pdf/1802.05522.pdf" class="uri">https://arxiv.org/pdf/1802.05522.pdf</a></p>
<ul>
<li>Main contribution: explicitly consider the inferred 3D geometry of the scene, enforcing consistency of the estimated 3D point clouds and ego-motion across consecutive frames - Directly penalize inconsistencies in the estimated depth without relying on BP - The validity mask is analytically computed rather than learned from data. <strong>comments: only out-of-view invalidity is considered here. In learning based validity mask prediction, occlusion and moving objects are also considered.</strong> - Learn from an uncalibrated video stream</li>
<li>The novel loss induced by ICP: - <span class="math inline">\(D_{t-1}\)</span> and <span class="math inline">\(D_t\)</span>: depth maps of two consecutive frames, from which two points can be constructed - <span class="math inline">\(Q_{t-1}\)</span> and <span class="math inline">\(Q_t\)</span>: corresponding point clouds by unprojection, represented in local camera frame - Apply the estimated ego-motion <span class="math inline">\(T_t\doteq T_{t-1\leftarrow t}\)</span> we get two roughly aligned point clouds <span class="math inline">\(\hat Q_{t-1}\doteq T_t Q_t\)</span> - Optimal transformation <span class="math inline">\(T&#39;_t\)</span> between the pair <span class="math inline">\((Q_{t}, \hat Q_{t-1})\)</span> can be found via ICP - We penalize two things: - Ideally <span class="math inline">\(T&#39;_t\)</span> should be identity matrix if the ego-motion prediction <span class="math inline">\(T_t\)</span> is perfect, thus we penalize its discrepancy to identity <span class="math inline">\(\| T&#39;_t - I\|_1\)</span><br />
- The point distance residual associated with the optimal transformation is also incorporated into the loss <span class="math inline">\(\|r_t\|_1\)</span> - The total 3D loss is <span class="math inline">\(L_\text{3D}=\|T&#39;_t-I\|1+\|r_t\|_1\)</span> <img src="2018-04-06-08-42-26.png" /></li>
<li>Photometric losses widely used in other depth prediction papers are also used: - SSIM (structure similarity) loss <span class="math inline">\(L_\text{SSIM}\)</span> - Disparity smoothness loss weighted by image gradient <span class="math inline">\(L_\text{smooth}\)</span> - Reconstruction loss (photometric error) <span class="math inline">\(L_\text{rec}\)</span> same as <span class="math inline">\(L_\text{ap}\)</span> in the left-right consistency paper. - For details about these losses, see the left-right consistency paper.</li>
</ul>
<hr />
<h3 id="v2v-posenet-voxel-to-voxel-prediction-network-for-accurate-3d-hand-and-human-pose-estimation-from-a-single-depth-map">V2V-PoseNet: Voxel-to-Voxel Prediction Network for Accurate 3D Hand and Human Pose Estimation from a Single Depth Map</h3>
<p><a href="https://arxiv.org/pdf/1711.07399.pdf" class="uri">https://arxiv.org/pdf/1711.07399.pdf</a></p>
<p>CVPR 18</p>
<ul>
<li>Existing works take a single 2D depth map and directly regresses the 3D coordinates of keypoints, which have two weaknesses: 1. presence of perspective distortion: The depth map is essentially 3D data, however when project the 3D shape to the image plane, perspective distortion is introduced to the depth map. The network is compelled to learn away such perspective distortion, which can be avoided if we feed volumetric data instead. 2. direct 3D coordinates regression from a 2D image is a highly non-linear mapping, which causes difficulty in the learning procedure.</li>
<li>This paper casts the 3D hand and human pose estimation problem from a single depth map into a voxel-to-voxel prediction that uses a 3D voxelized grid and estimates the per-voxel likelihood for each keypoint.</li>
<li>The method, though simple, outperforms prior art on almost all the benchmarks.</li>
</ul>
<hr />
<h3 id="real-time-monocular-depth-estimation-using-synthetic-data-with-domain-adaptation-via-image-style-transfer">Real-Time Monocular Depth Estimation using Synthetic Data with Domain Adaptation via Image Style Transfer</h3>
<p><a href="http://breckon.eu/toby/publications/papers/abarghouei18monocular.pdf" class="uri">http://breckon.eu/toby/publications/papers/abarghouei18monocular.pdf</a></p>
<p>CVPR 18</p>
<hr />
<h3 id="megadepth-learning-single-view-depth-prediction-from-internet-photos">MegaDepth: Learning Single-View Depth Prediction from Internet Photos</h3>
<p><a href="https://arxiv.org/pdf/1804.00607.pdf" class="uri">https://arxiv.org/pdf/1804.00607.pdf</a></p>
<p>CVPR 18</p>
<ul>
<li>Use multi-vew Internet photo collections to generate training data via modern structure-from-motion (SfM) and multi-view stereo (MVS) methods, and present MegaDepth -- a large depth dataset. - Motivation: learning based methods need pairs of RGB image and depth map, which is difficult to collect: - RGB-D: restricted to indoor scenes - Laser scanners: sparse depth and hard to operate - crowdsourcing: only for sparse ordinal relations or surface normals (Weifeng's work) - Difficulty: there are some hard cases for SfM and MVS (take COLMAP as the underlying MVS system) - dynamic objects (people, cars, etc.): cannot establish reliable correspondences for SfM or MVS - noisy depth discontiinuities: object boundaries - bleeding of background depths into foreground objects: in classic formulation of SfM and MVS, depth estimation is regularized to be consistent (smooth) which can be achieved by consistently predicting the background depth. - Ways to overcome bad preliminary depth reconstruction from COLMAP: 1. prefer less training data over bad data training data: throw away outlier depth maps 2. utilize semantic segmentation (PSPNet CVPR17 is used) to enhance and filter the depth maps, and to yield large amounts of ordinal depth comparisons as additional training data.</li>
<li>Depth enhancement via semantic segmentation - Images with high-quality depth reconstruction (with high fraction of valid depth reconstruction) are used for training of Euclidean depth. - Images with potentially big regions of dynamic objects (such as people, cars) are used to learn ordinal depth relationships.</li>
<li>Network: VGG, ResNet and hourglass (HG) architectures are compared -- HG performs best</li>
<li><p>Loss: - Depth reconstruction from SfM and MVS are up to a scale, use scale-invariant loss from Eigen and Fergus: <span class="math display">\[
      L_\text{data}=\frac{1}{n}\sum_{i=1}^n (R_i)^2 - \frac{1}{n^2}\big(\sum_{i=1}^n R_i\big)^2
      \]</span> where n is the number of valid depths in the ground truth depth map. <span class="math inline">\(R_i=L_i - L_i^*\)</span> is the difference of log-depth between prediction <span class="math inline">\(L_i\)</span> and ground truth <span class="math inline">\(L_i^*\)</span>. This loss is essentially the empirical variance of the log-depth difference. - Multi-scale scale-invariant gradient matching term: to encourage smooth depth prediction and preserve sharp depth discontinuties at object boundaries.</p>
<pre><code>$$
L_\text{grad}=\frac{1}{n}\sum_{k}\sum_{i}(|\nabla_x R_i^k| +|\nabla_y R_i^k|)
$$
where k indicates scale level.
- Robust ordinal depth loss:

$$
L_\text{ord}=\begin{cases}
\log(1+\exp(P_{ij})), &amp;\quad\text{if } P_{ij} \le \tau\\
\log(1+\exp(\sqrt{P_{ij}}))+c, &amp;\quad\text{if } P_{ij} &gt; \tau
\end{cases}
$$
where $P_{ij}=-r_{ij}^*(L_i-L_j)$ and $r_{ij}^*$ is the ground truth ordinal depth relation between i and j (r=1 if i is further than j and -1 otherwise) and c is a constant to ensure $L_\text{ord}$ is continuous.</code></pre></li>
<li></li>
<li><p>Main references: - multi view 3d model learning: [26] Learning 3d object categories by looking around them. ICCV, 17. - multi view scene depth estimation: Demon: Depth and Motion network for learning monocular stereo. CVPR, 17.</p></li>
</ul>
<hr />
<h3 id="learning-depth-from-monocular-videos-using-direct-methods">Learning Depth from Monocular Videos using Direct Methods</h3>
<p><a href="https://arxiv.org/pdf/1712.00175.pdf" class="uri">https://arxiv.org/pdf/1712.00175.pdf</a></p>
<p>CVPR 18</p>
<ul>
<li>Previous works on learning based joint pose and depth prediction has two separate modules: one pose predictor and one depth predictor, though trained jointly, the geometric relation between the two is not fully explored. - the pose network might be unnecessary and sub-optimal compared to the proposed method.</li>
<li>Essentially, given two images and the depth prediction, one can find the optimal relative pose upto the precision of the depth prediction by minimizing the photometric error directly which is what direct visual odometry (DVO) does. - To enable direct optimization of pose, the paper introduces a differentiable direct visual odometry (DDVO) module. - Incorporating DDVO in an end-to-end learning framework requires gradients of matrix pseudoinverse operations, which is carried out multiple times in the iterative solver of the minimization problem. Regarding the gradients of matrix pseudoinverse, see [29]. - Also solving the non-linear minimization problem involves per iteration linearization of the objective at the current estimate. To avoid linearization multiple times, the <em>Inverse Compositional Algorithm</em> [3] which reverses the roll of source image and reference image and compute the parameters update for the reference image such that the Jacobian and Hessian matrix do not need to be re-evaluated per iteration.</li>
</ul>
<div class="figure">
<img src="monodepth_direct_net.png" />

</div>
<p>To train a depth prediction network in unsupervised way, one typically solves the following minimization problem:</p>
<p><span class="math display">\[
\min_{\theda_d, p} L_\text{ap}(f_d(I;\theta_d), p) + L_\text{prior}(f_d(I; \theta_d))
\]</span> where <span class="math inline">\(f_d\)</span> is depth predictor and <span class="math inline">\(\theta_d\)</span> denotes the network parameters, <span class="math inline">\(p\)</span> is relative pose and <span class="math inline">\(I\)</span> is image. What this equation says is: One wants to minimze the appearance difference by warping images according to relative pose and predicted depth (1st term) with some regularization term on predicted depth such as smootheness (2nd term).</p>
<p>The following minimization represents the two-separate-network scheme:</p>
<p><span class="math display">\[
\min_{\theta_d} \min_p L_\text{ap} L_\text{ap}(f_d(I;\theta_d), p) + L_\text{prior}(f_d(I; \theta_d))
\]</span></p>
<p>which is adopted by Tingyu Zhou CVPR 17.</p>
<p>And this paper proposes the following scheme:</p>
<p><span class="math display">\[
\min_{\theta_d} L_\text{ap}(f_d(I;\theta_d), f_p(f_d(I;\theta_d))) + L_\text{prior}(f_d(I; \theta_d))
\]</span></p>
<p>where <span class="math inline">\(f_p\)</span> is the DDVO (a deterministic function) module which computes the optimal pose upto predicted depth. This module is parameter-free and thus does not require training.</p>
<ul>
<li>Another important contribution of this paper is the depth normalization trick: To make the loss function less sensitive to scale, normalize the (inverse) depth before feeding it to loss, which is also used by LSD-SLAM for keyframe selection:</li>
</ul>
<p><span class="math display">\[
\mu(d_i)\doteq \frac{N d_i}{\sum_{j=1}^N d_j}
\]</span></p>
<p>for stereo based monocular depth prediction, this might be unnecessary since the baseline of the stereo head already provides the metric scale for depth prediction. - FIXME: study the inverse compositional method. How per iteration linearization is avoided? - Misc: The second order minimization problem is sensitive to initialization (since 2nd-order methods model the objective as a quadratic function which is only valid in a small neighborhood of the solution), using output of Pose-CNN as initialization performs best. - multi-scale prediction and loss - 2nd smoothness: Instead of using 1st order image gradient to weight depth regularization, they use image Laplacian for the weight and 2nd derivative of depth for smooth regularization. - The analysis on scale ambiguity in monocular trained monocular depth estimation is valuable: Photometric error is scale invariant; but for the regularization (prior) term, small inverse depth (or large depth) is preferred by the loss since one can always decrease the smoothness term by scaling down the prediction. - Main references: - gradients of matrix pseudoinverse: <a href="http://www.ci2cv.net/media/papers/deepLK-icra.pdf">29</a> Deep-KL for efficient adaptive object tracking. arXiv, 17. - inverse compositional algorithm: <a href="https://www.ri.cmu.edu/pub_files/pub3/baker_simon_2002_3/baker_simon_2002_3.pdf">3</a>Lucas-kanade 20 years on: A unifying framework. IJCV, 04. - application of inverse compositional alg. in deep learing: - [4]: CLKN: Cascaded Lucas-Kanade networks for image alignment. CVPR, 17. - [22]: Inverse compositional spatial transformer networks. 2017.</p>
<hr />
<h3 id="inverse-compositional-spatial-transformer-networks">Inverse Compositional Spatial Transformer Networks</h3>
<p><a href="https://github.com/chenhsuanlin/inverse-compositional-STN" class="uri">https://github.com/chenhsuanlin/inverse-compositional-STN</a></p>
<p><a href="https://chenhsuanlin.bitbucket.io/inverse-compositional-STN/paper.pdf" class="uri">https://chenhsuanlin.bitbucket.io/inverse-compositional-STN/paper.pdf</a></p>
<p>CVPR 17</p>
<ul>
<li>Motivation: a method to learn representation invariant to spatial variation. Two existing approaches to handle spatial: data augmentation, spatial pooling and spatial transformer networks (STN): - data augmentation and : require an exponentional increase in the number of training samples and thus the capacity of the model to be learned. (extra burden to be spatial variation robust if not completely invarant yet) - spatial pooling: can only handle small spatial variation due to the receptive field of each pooling operation - STN: explicitly model the spatail transformation, can handle large spatial variation with a small amount of parameters. Problem is STNs directly apply the predicted transformation to warp the feature maps and pass through the warped feature maps, which has boundary effects (in zoom out type transformation).</li>
<li>Idea: IC-STN passes around the warp parameters instead of the warped feature maps compared to STN. - The idea of passing around the warp parameters is inspired by Lucas-Kanade algorithm which solves image alignment problem by minimizing the photometric error in an iterative way where at each iteration, the predicted warp is applied to the source image from which, together with the target image, an update is computed. - Essentially, this is only loosely related to LK and related to a lot of nonlinear least sqaure solvers.</li>
</ul>
<h4 id="lucas-kanade-and-the-inverse-compositional-algorithm">Lucas-Kanade and the Inverse Compositional algorithm</h4>
<p>Given source image <span class="math inline">\(I\)</span> and target/template image <span class="math inline">\(T\)</span>, LK computes a warp to align the two in the sense that the following squared differences (SSD) objective is minimized:</p>
<p><span class="math display">\[
\min_{\Delta p}\|I(p+\Delta p) - T(0)\|_2^2
\]</span></p>
<p>we denote <span class="math inline">\(I(p)\)</span> as the source image <span class="math inline">\(I\)</span> warped with the parameters <span class="math inline">\(p\)</span>. The LK algorithm solves this problem by iterating the following two steps: first linearize the non-linear LS at the current estimate and then solve a linear LS problem, whose solution is used to update the parameter which is then used in the next iteration.</p>
<ul>
<li>Linearize at the current estimate by Taylor expansion:</li>
</ul>
<p><span class="math display">\[
\min_{\Delta p}\|I(p)+\frac{\partial I(p)}{\partial p} \Delta p - T(0)\|_2^2
\]</span></p>
<p>The problem of the original LK algorithm is that at each iteration, one needs to linearize the objective at the latest estimate. The Inverse Compositional (IC) algorithm is an efficient varaint of LK which instead of linearizes the source image at current estimate but linearizes the target image at identity element of the transformation group.</p>
<p><span class="math display">\[
\min_{\Delta p}\|I(p)-T(0)-\frac{\partial T(0)}{\partial p}\Delta p\|_2^2
\]</span></p>
<p>LS solution is given by the following normal equation:</p>
<p><span class="math display">\[
\Delta p=\big(J^\top J\big)^{-1}J^\top (I(p)-T(0))
\]</span></p>
<p>where <span class="math inline">\(J=\frac{\partial T(0)}{\partial p}\)</span> is the Jacobian matrix of target image at identity group element. The warp parameter is then updated as <span class="math inline">\(p \leftarrow p \circ (\Delta p)^{-1}\)</span>. <span class="math inline">\(\circ\)</span> is the compose operator of the underlying transformation group.</p>
<ul>
<li><p>For the recurrent IC-STN structure, see Fig. 5 of the paper. In practice the recurrent structure is unfold into several stages as shown in Fig 4, where network parameters are shared among the warp parameter predictors. - In theory one needs to predict a warp, invert it and then apply the inverse warp to current warp. - In practice, for end-to-end learning framework, it's better to directly predict the inverse warp and then apply it to the current estimate.</p></li>
<li><p>FIXME: study the code.</p></li>
</ul>
<hr />
<h3 id="clkn-cascaded-lucas-kanade-networks-for-image-alignment.-cvpr-17.">CLKN: Cascaded Lucas-Kanade networks for image alignment. CVPR, 17.</h3>
<p><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Chang_CLKN_Cascaded_Lucas-Kanade_CVPR_2017_paper.pdf" class="uri">http://openaccess.thecvf.com/content_cvpr_2017/papers/Chang_CLKN_Cascaded_Lucas-Kanade_CVPR_2017_paper.pdf</a></p>
<p>CVPR 17</p>
<hr />
<h3 id="geonet-unsupervised-learning-of-dense-depth-optical-flow-and-camera-pose">GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose</h3>
<p><a href="https://arxiv.org/pdf/1803.02276.pdf" class="uri">https://arxiv.org/pdf/1803.02276.pdf</a></p>
<p>CVPR 18</p>
<ul>
<li>Main references: - cost volume for stereo matching: [24] End-to-end learning of geometry and context for deep stereo regression. - optical flow using rigid object prior: [54] Optical flow in mostly rigid scenes.</li>
</ul>
<hr />
<h3 id="unsupervised-learning-of-monocular-depth-estimation-and-visual-odometry-with-deep-feature-reconstruction">Unsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction</h3>
<p><a href="https://arxiv.org/pdf/1803.03893.pdf" class="uri">https://arxiv.org/pdf/1803.03893.pdf</a></p>
<p>CVPR 18</p>
<ul>
<li>Difference from the left-right consistency paper: - In addition to left-right image pairs, also consider temporal image pairs. - In addition to the photometric reconstruction loss, also incorporate a feature reconstruction loss into the total loss where feature maps at early conv layers are compared. - In addition to depth prediction, relative pose between two temporally consecutive images is learned in a <em>semi-supervised</em> fashion in the sense that: the stereo (left-right) pair provides ground truth metric pose and metric pose of two temporally consecutive frames is unknown in training.</li>
<li>Loss: <span class="math inline">\(L\text{total}=\lambda_{ir}L_\text{ir} + \lambda_\text{fr}L_\text{fr} + \lambda_\text{ds}L_\text{ds}\)</span>, where ir is image reconstruction, fr is feature reconstruction, ds is disparity smoothness. For details, see the paper.</li>
<li>Network architecture: <img src="featrec_net.png" /></li>
<li>Performance: - Depth: slightly better than lr consistency paper. - Pose: better than Tingyu Zhou CVPR17, also better than ORB-SLAM on certain KITTI sequences (dont know why ORB-SLAM performs so poorly on these sequences)</li>
<li>Illustration of spatial and temporal consistency between image pairs: <img src="featrec_illustration.png" /></li>
</ul>
<hr />
<h3 id="unsupervised-monocular-depth-estimation-with-left-right-consistency">Unsupervised Monocular Depth Estimation with Left-Right Consistency</h3>
<p><a href="https://arxiv.org/pdf/1609.03677.pdf" class="uri">https://arxiv.org/pdf/1609.03677.pdf</a></p>
<p>CVPR 17</p>
<ul>
<li>Loss: The total loss is the sum of losses at each output scale: <span class="math inline">\(C=\sum_{s\in S} C_s\)</span>, where <span class="math inline">\(S\)</span> is the set of scale levels, for instance <span class="math inline">\(S \doteq \{1, 2,\cdots, 4\}\)</span>. The loss <span class="math inline">\(C_s\)</span> at scale <span class="math inline">\(s\)</span> is defined as:</li>
</ul>
<p><span class="math display">\[
C_s=\alpha_{ap}(C_{ap}^L+C_{ap}^R) + \alpha_{ds}(C_{ds}^L+C_{ds}^R) + \alpha_{lr}(C_{lr}^L+C_{lr}^R)
\]</span> where superscript <span class="math inline">\(L, R\)</span> indicates the image on which loss is computed. Basically the network only takes the left image, but predicts both left and right disparities, i.e., <span class="math inline">\(d^L\)</span> and <span class="math inline">\(d^R\)</span>. We warp the left image to the right by applying <span class="math inline">\(d^L\)</span> and compute the (left) loss (with superscript L). Similarly, we warp the right image to the left by applying <span class="math inline">\(d^R\)</span> and compute the (right) loss.</p>
<table>
<thead>
<tr class="header">
<th align="left">loss</th>
<th align="left">meaning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(C_{ap}\)</span></td>
<td align="left">appearance similarity</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(C_{ds}\)</span></td>
<td align="left">disparity smoothness</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(C_{lr}\)</span></td>
<td align="left">left-right consistency</td>
</tr>
</tbody>
</table>
<ul>
<li>Photometric Image Loss: The photometric image reconstruction loss <span class="math inline">\(C_{ap}\)</span> is comprised of two terms: <span class="math inline">\(l_1\)</span> reconstruction error and single scale SSIM</li>
</ul>
<p><span class="math display">\[
C_{ap}^L=\frac{\alpha}{|\Omega|} 
\sum_{\omega \in \Omega} 
  \frac{1-\mathrm{SSIM}(I_\omega, \tilde I_\omega)}
  {2} + 
  (1-\alpha) \| I_\omega-\tilde I_\omega\|_1
\]</span></p>
<p>where SSIM reads as:</p>
<p><span class="math display">\[
\mathrm{SSIM}(x, y) = 
\frac{(2\mu_x\mu_y+C_1)(2\sigma_{xy}+C_2)}
{(\mu_x^2+\mu_y^2+C_1)(\sigma_x^2+\sigma_y^2+C_2)}
\]</span></p>
<p>and a simplified SSIM with a <span class="math inline">\(3\times 3\)</span> block filter is used instead of a Gaussian. <span class="math inline">\(\alpha=0.85\)</span>. - Disparity Smootheness Loss: Encourage disparities to be locally smooth with an <span class="math inline">\(l_1\)</span> penalty on disparity gradients <span class="math inline">\(\nabla d\)</span>. As depth discontinuities often occur at image gradients, the disparity smoothness is weighted by an edge-aware term using the image gradients <span class="math inline">\(\nabla I\)</span>:</p>
<p><span class="math display">\[
C_{ds}^l= \frac{1}{|\Omega|}
\sum_{\omega \in \Omega} 
|\nabla_x d_\omega| \exp(-|\nabla_x I_\omega|) + |\nabla_y d_\omega|\exp(-|\nabla_y I_\omega|)
\]</span></p>
<ul>
<li>Left-Right Disparity Consistency Loss: Both left and right disparities are predicted given ONLY the left image. This cost attempts to make the left-view disparity map be equal to the projected right-view disparity map.</li>
</ul>
<p><span class="math display">\[
C_{lr} = \frac{1}{|\Omega|}\sum_{\omega \in \Omega} \|d_\omega^L-d_{\omega + d_\omega^L}^R\|_1
\]</span></p>
<hr />
<h3 id="d-rcnn-instance-level-3d-object-reconstruction-via-render-and-compare">** 3D-RCNN: Instance-level 3D Object Reconstruction via Render-and-Compare</h3>
<p>Abhijit Kundu, Yin Li, and James M. Rehg.</p>
<p>To appear CVPR 18.</p>
<hr />
<h3 id="feature-space-optimization-for-semantic-video-segmentation">Feature Space Optimization for Semantic Video Segmentation</h3>
<p>Abhijit Kundu, Vibhav Vineet, Vladlen Koltun. CVPR16.</p>
<hr />
<h2 id="the-basics">The Basics</h2>
<h3 id="spatial-transformer-networks">Spatial Transformer Networks</h3>
<ul>
<li>The spatial transformer mechanism consists of three components:
<ol style="list-style-type: decimal">
<li>Localization network: takes the input feature map and outputs the parameters of the spatial transformation.</li>
<li>Grid generator: takes the parameters of the spatial transformation and generates a sampling grid, based on which the input feature will be mapped to the output feature map, which is done by the <em>sampler</em>.</li>
<li>Sampler: takes the input feature map and the sampling grid from step 1 &amp; 2 and produces the output feature map.</li>
</ol></li>
</ul>
<p><img src="./STN.png" alt="alt text" /> - Localization network: <span class="math inline">\(\theta=f_\text{loc}(U)\)</span> where <span class="math inline">\(U\in \mathbb{R}^{H\times W \times C}\)</span> is the input feature map and <span class="math inline">\(\theta\)</span> is the parameter of the spatial transformation. For instance, if the transformation is affine, <span class="math inline">\(\theta\)</span> should have 6 paramters. - Sampling grid: Let <span class="math inline">\(V\in\mathbb{R}^{H&#39;\times W&#39;\times C}\)</span> be the output feature map and <span class="math inline">\(G=\{G_i\}\)</span> be a regular grid of the output feature map each <span class="math inline">\(G_i=(x_i^t, y_i^t)\)</span> (superscript <span class="math inline">\(t\)</span> denotes target/output while <span class="math inline">\(s\)</span> denotes source/input). With affine transformation as an example, the relation between input grid and output grid is: <span class="math display">\[
\begin{pmatrix}
x_i^s\\
y_i^s
\end{pmatrix}
=
\mathcal{T}_\theta(G_i)
=
A_\theta
\begin{pmatrix}
x_i^t\\
y_i^t\\
1
\end{pmatrix}
=
\begin{bmatrix}
\theta_{11} &amp; \theta_{12} &amp; \theta_{13}\\
\theta_{21} &amp; \theta_{22} &amp; \theta_{23}
\end{bmatrix}
\begin{pmatrix}
x_i^t\\
y_i^t\\
1
\end{pmatrix}
\]</span></p>
<ul>
<li>Differentiable sampler: each <span class="math inline">\((x_i^s, y_i^s)\)</span> in <span class="math inline">\(\mathcal{T}_\theta(G)\)</span> defines the sptial location in the input <span class="math inline">\(U\)</span> where a sampling kernel is applied to get the value at a particular pixel in the output <span class="math inline">\(V\)</span>.</li>
</ul>
<p><span class="math display">\[
V_i^c=\sum_n^H\sum_m^W U_{nm}^c k(x_i^s-m; \Phi_x)k(y_i^s-n;\Phi_y)\quad \forall i \in [1\cdots H&#39;W&#39;] \quad c\in [1\cdots C]
\]</span> where <span class="math inline">\(\Phi_x\)</span> and <span class="math inline">\(\Phi_y\)</span> are the parameters of a generic sampling kernel <span class="math inline">\(k()\)</span>, which defines the image interpolation (bilinear, cubic, nearest neighbor ...). <span class="math inline">\(U_{nm}^c\)</span> is the value at location <span class="math inline">\((n,m)\)</span> in channel <span class="math inline">\(c\)</span> of the input, and <span class="math inline">\(V_i^c\)</span> is the output value for pixel <span class="math inline">\(i\)</span> at location <span class="math inline">\((x_i^t, y_i^t)\)</span> in channel <span class="math inline">\(c\)</span>. In theory, any interpolation method can be used as long as the (sub)gradient of the operation can be computed. In case of bilinear interpolation:</p>
<p><span class="math display">\[
V_i^c=\sum_n^H\sum_m^W U_{nm}^c \max(0, 1-|x_i^s-m|)\max(0,1-|y_i^s-n|)
\]</span> and gradients:</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\frac{\partial V_i^c}{\partial U_{nm}^c}=\max(0, 1-|x_i^s-m|)\max(0, 1-|y_i^s-n|)\\
&amp;\frac{\partial V_i^c}{\partial x_i^s}
=\sum_n^H\sum_m^W U_{nm}^c\max(0, 1-|y_i^s-n|)
\begin{cases}
0 &amp;\text{if } &amp;|m-x_i^s| \ge 1\\
1 &amp;\text{if } &amp;m \ge x_i^s\\
-1 &amp;\text{if } &amp;m &lt; x_i^s
\end{cases}
\end{aligned}
\]</span> and similar for <span class="math inline">\(\frac{\partial V_i^c}{\partial y_i^s}\)</span>.</p>
<hr />
<h3 id="generative-adversarial-nets">Generative Adversarial Nets</h3>
<p><a href="https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf" class="uri">https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf</a></p>
<hr />
<h3 id="auto-encoding-variational-bayes">Auto-Encoding Variational Bayes</h3>
<p><a href="https://arxiv.org/pdf/1312.6114.pdf" class="uri">https://arxiv.org/pdf/1312.6114.pdf</a></p>
<hr />
<h3 id="stochastic-backpropagation-and-approximate-inference-in-deep-generative-models">Stochastic Backpropagation and Approximate Inference in Deep Generative Models</h3>
<p><a href="https://arxiv.org/pdf/1401.4082.pdf" class="uri">https://arxiv.org/pdf/1401.4082.pdf</a></p>
<hr />
<h3 id="an-introduction-to-variational-methods-for-graphical-models">An Introduction to Variational Methods for Graphical Models</h3>
<p><a href="https://people.eecs.berkeley.edu/~jordan/papers/variational-intro.pdf" class="uri">https://people.eecs.berkeley.edu/~jordan/papers/variational-intro.pdf</a></p>
<h4 id="exact-inference">Exact inference</h4>
<ul>
<li>The junction tree algorithom compiles directed graphical models into undirected graphical models in two steps: 1. moralization: convert the directed graph into an undirected graph(marry parents of each node and remove the direction of edges). 2. triangulation: add additional edges to the moral graph such that recursive calculation of probabilities can be performed on the resulting graph</li>
<li>Subsequent inferential calculation is carried out in the undirected formalism, whose time complexity depends on <em>the size of the cliques</em>; - in particular for discrete data the number of values required to represent the potential is exponential in the number of nodes in the clique. - For efficient inference, it is therefore critical to obtain small cliques.</li>
</ul>
<h4 id="examples">Examples</h4>
<ul>
<li>QMR-DT (disease) database: <span class="math inline">\(d\)</span> is disease and <span class="math inline">\(f\)</span> is sympton (feature). Both are binary random variables.</li>
</ul>
<p><span class="math display">\[
P(f,d)=P(f|d)P(d)=\big(\prod_i P(f_i|d)\big) \big(\prod_j P(d_j) \big)
\]</span> where <span class="math inline">\(P(f_i=0|d)=\exp\big(-\sum_{j\in \pi_i} \theta_{ij}d_j - \theta_{i0}\big)\)</span> and <span class="math inline">\(P(f_i=1|d)=1-\exp\big(-\sum_{j\in \pi_i} \theta_{ij}d_j - \theta_{i0}\big)\)</span>.</p>
<ul>
<li>Neural Networks as graphical models: use sigmoid as activation and treat the activation as the probability of the node beging in state 1. Each node is treated as a binary random variable.</li>
</ul>
<p><span class="math display">\[
p(S_i=1|S_{\pi_i}) = \frac{1}{1+\exp(-\sum_{j\in\pi_i} \theta_{ij}S_j - \theta_{i0})}
\]</span></p>
<ul>
<li>Bolzmann machines: the clique potentials are formed by taking products of Bolzmann factors -- exponential of terms that are at most quadratic in the <span class="math inline">\(S_i \in \{0, 1\}\)</span>. A given pair of nodes can <em>appear in multiple, overlapping cliques</em>. For each such pair we assume that <em>each distinct expression appears as a factor in one and only one clique potential</em>.</li>
</ul>
<p><span class="math display">\[
P(S) = \frac{1}{Z}\exp(\sum_{i&lt; j} \theta_{ij}S_iS_j + \sum_{i}\theta_{i0}S_i)
\]</span></p>
<p>where <span class="math inline">\(\theta_{ij}=0\)</span> means nodes i and j are not neighbors in the graph. Negative of the exponent in above equation is called <em>energy</em> and a distribution with the above density is called a <em>Bolzmann distribution</em>.</p>
<h4 id="variational-inference">Variational inference</h4>
<ul>
<li>Sequential methods: One can sequentially replace some nodes in the original graph with emergent nodes from approximation (via convex duality).</li>
<li>Block methods: One can also use a distribution family with good properties to approximate the original distribution.</li>
<li>Examples of using such approximate inference methods are given in the paper.</li>
<li>See the paper for details.</li>
</ul>
<hr />
<h3 id="uncertainty-in-deep-learning">Uncertainty in Deep Learning</h3>
<p><a href="http://mlg.eng.cam.ac.uk/yarin/thesis/thesis.pdf" class="uri">http://mlg.eng.cam.ac.uk/yarin/thesis/thesis.pdf</a></p>
<p>PhD Thesis</p>
</body>
</html>
